{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhFrJ-CHZDvU"
   },
   "source": [
    "# Default title text\n",
    "# RAG System - Asistente de Inversión en Dividendos\n",
    "\n",
    "Este notebook implementa un sistema completo de **Retrieval-Augmented Generation (RAG)** especializado en inversión en dividendos.\n",
    "\n",
    "## 🎯 Objetivo\n",
    "Desarrollar un asistente inteligente capaz de responder preguntas específicas sobre estrategias de inversión en dividendos, análisis financiero y gestión de carteras, basándose en un extenso corpus de conocimiento especializado.\n",
    "\n",
    "## 📚 Dataset de Conocimiento Financiero\n",
    "- **93 documentos** de contenido financiero especializado\n",
    "- **Pipeline de procesamiento avanzado**: Audio → Transcripción → Optimización con IA → Textos estructurados\n",
    "- **Áreas de conocimiento**: Análisis fundamental, selección de brokers, psicología de la inversión, gestión de riesgo, fiscalidad, ratios financieros, etc.\n",
    "- **Estructura modular**: Contenido organizado en 16 módulos temáticos + material complementario\n",
    "\n",
    "## 🔧 Stack Tecnológico\n",
    "- **LangChain**: Framework para aplicaciones RAG\n",
    "- **OpenAI**: Embeddings (text-embedding-3-large) y LLM (GPT-4o-mini)\n",
    "- **FAISS**: Base de datos vectorial para búsqueda semántica eficiente\n",
    "- **Python**: Procesamiento de datos y análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kze0ZANvZMXU"
   },
   "source": [
    "## 📦 Instalación de Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCvCmBoxYzYL"
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain-openai langchain-community faiss-cpu tiktoken tqdm pandas matplotlib seaborn openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqObnBg0gwsH"
   },
   "source": [
    "## 💾 Configuración de Google Drive y Verificación de Chunks\n",
    "\n",
    "Montamos Google Drive para persistir los chunks procesados y evitar repetir el proceso de extracción de metadatos con GPT-4o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zz5IwrdKgwsH"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "print(\"📁 Montando Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "print(\"✅ Google Drive montado correctamente\")\n",
    "\n",
    "DRIVE_FOLDER = '/content/drive/MyDrive/RAG_Dividendos/'\n",
    "CHUNKS_PATH = '/content/drive/MyDrive/RAG_Dividendos/chunks_procesados.pkl'\n",
    "\n",
    "os.makedirs(DRIVE_FOLDER, exist_ok=True)\n",
    "print(f\"📂 Carpeta de trabajo: {DRIVE_FOLDER}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔍 Verificar si ya existen chunks procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJJliBcTgwsI"
   },
   "outputs": [],
   "source": [
    "print(\"🔍 Verificando chunks existentes en Google Drive...\")\n",
    "\n",
    "chunks_exist = os.path.exists(CHUNKS_PATH)\n",
    "\n",
    "if chunks_exist:\n",
    "    print(\"✅ ¡Chunks encontrados en Google Drive!\")\n",
    "    print(f\"📁 Ruta: {CHUNKS_PATH}\")\n",
    "\n",
    "    try:\n",
    "        print(\"📥 Cargando chunks desde Google Drive...\")\n",
    "        with open(CHUNKS_PATH, 'rb') as f:\n",
    "            chunks = pickle.load(f)\n",
    "\n",
    "        print(f\"🎉 ¡Chunks cargados exitosamente!\")\n",
    "        print(f\"   📊 Total de chunks: {len(chunks)}\")\n",
    "        print(f\"   📚 Documentos únicos: {len(set([chunk.metadata['source_file'] for chunk in chunks]))}\")\n",
    "\n",
    "        if chunks:\n",
    "            sample_chunk = chunks[0]\n",
    "            has_metadata = 'main_topic' in sample_chunk.metadata and 'level' in sample_chunk.metadata\n",
    "\n",
    "            if has_metadata:\n",
    "                print(f\"✅ Metadatos verificados - chunks listos para usar\")\n",
    "                print(f\"\\n📋 Ejemplo de chunk cargado:\")\n",
    "                print(f\"   🆔 Chunk ID: {sample_chunk.metadata.get('chunk_id', 'N/A')}\")\n",
    "                print(f\"   📁 Archivo: {sample_chunk.metadata.get('filename', 'N/A')}\")\n",
    "                print(f\"   🎯 Tema: {sample_chunk.metadata.get('main_topic', 'N/A')}\")\n",
    "                print(f\"   📊 Nivel: {sample_chunk.metadata.get('level', 'N/A')}\")\n",
    "                print(f\"   🔤 Tokens: {sample_chunk.metadata.get('chunk_tokens', 'N/A')}\")\n",
    "\n",
    "                print(f\"\\n⚡ SALTANDO TODO EL PROCESAMIENTO - chunks listos para RAG\")\n",
    "                SKIP_PROCESSING = True\n",
    "            else:\n",
    "                print(f\"⚠️ Chunks encontrados pero sin metadatos completos\")\n",
    "                print(f\"🔄 Se ejecutará el procesamiento completo\")\n",
    "                SKIP_PROCESSING = False\n",
    "        else:\n",
    "            print(f\"❌ Archivo de chunks vacío\")\n",
    "            SKIP_PROCESSING = False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error cargando chunks: {e}\")\n",
    "        print(f\"🔄 Se ejecutará el procesamiento completo\")\n",
    "        SKIP_PROCESSING = False\n",
    "        chunks = None\n",
    "\n",
    "else:\n",
    "    print(\"❌ No se encontraron chunks en Google Drive\")\n",
    "    print(f\"🔄 Se ejecutará el procesamiento completo desde cero\")\n",
    "    SKIP_PROCESSING = False\n",
    "    chunks = None\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "if SKIP_PROCESSING:\n",
    "    print(\"🚀 MODO RÁPIDO: Usando chunks existentes\")\n",
    "    print(\"💡 Puedes saltar directamente a la implementación del RAG\")\n",
    "else:\n",
    "    print(\"🔄 MODO PROCESAMIENTO: Generando chunks desde cero\")\n",
    "    print(\"💡 Ejecuta las siguientes celdas para procesar el dataset\")\n",
    "print(f\"{'='*50}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljaBkIW4ZWCD"
   },
   "source": [
    "## 🔑 Configuración de API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dozJZCoCYzYM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Introduce tu OpenAI API Key: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7IkrzfpZaaz"
   },
   "source": [
    "## 📊 Análisis del Dataset de Conocimiento Financiero\n",
    "\n",
    "En esta sección analizaremos la estructura y características del dataset procesado, que contiene conocimiento especializado sobre inversión en dividendos.\n",
    "\n",
    "### 🎵 Pipeline de Procesamiento del Dataset\n",
    "\n",
    "Antes de proceder con el análisis, es importante entender cómo se ha generado este dataset de conocimiento financiero:\n",
    "\n",
    "#### **1. 🎙️ Extracción de Audio a Texto**\n",
    "- **Fuente original**: Contenido de audio especializado en inversión en dividendos\n",
    "- **Herramienta utilizada**: OpenAI Whisper para transcripción automática\n",
    "- **Resultado**: Transcripciones en bruto de los audios\n",
    "\n",
    "#### **2. ✂️ Segmentación Inicial**\n",
    "- **Objetivo**: Dividir el contenido en fragmentos manejables\n",
    "- **Límite**: Respetar los límites de tokens de la API de OpenAI\n",
    "- **Método**: Chunking preservando contexto (4.000 tokens - 300 overlap tokens) - (GPT-4o ~4.096 tokens máximos en la respuesta)\n",
    "\n",
    "#### **3. 🧠 Limpieza y Optimización con IA**\n",
    "- **Herramienta**: GPT-4o\n",
    "- **Proceso**: Refinamiento del contenido transcrito para mejorar claridad y estructura\n",
    "- **Prompt utilizado**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "source": [
    "Eres un redactor profesional especializado en convertir transcripciones de voz en textos escritos claros, concisos y de alta calidad para publicaciones financieras.\n",
    "\n",
    "Tienes como entrada la transcripción de una presentación sobre inversión en dividendos. Tu tarea es reescribir el texto para que:\n",
    "\n",
    "ELIMINAR COMPLETAMENTE:\n",
    "- Muletillas, repeticiones, frases interrumpidas y expresiones orales (como \"ehh\", \"vale\", \"como decíamos\", \"bueno\", \"entonces\").\n",
    "- Referencias a cursos, módulos, lecciones, clases, alumnos, estudiantes o cualquier contexto educativo.\n",
    "- Instrucciones técnicas de plataformas (como \"haz clic\", \"descarga\", \"deja tu comentario\").\n",
    "- Expresiones dirigidas directamente al lector (\"tú\", \"vosotros\", \"ustedes\").\n",
    "- Markdown innecesario, emojis decorativos, saltos de línea excesivos (`<br>`).\n",
    "\n",
    "TRANSFORMAR EN:\n",
    "- Un texto profesional de divulgación financiera, como si fuera parte de un libro especializado o artículo de revista económica.\n",
    "- Redacción en tercera persona o impersonal, con tono objetivo y profesional.\n",
    "- Estructura clara con párrafos bien organizados.\n",
    "- Terminología técnica precisa sin simplificaciones excesivas.\n",
    "\n",
    "MANTENER:\n",
    "- Toda la información técnica y financiera relevante sobre dividendos.\n",
    "- Datos, cifras, ejemplos prácticos y estrategias de inversión.\n",
    "- Conceptos, definiciones y explicaciones técnicas.\n",
    "\n",
    "MANEJO DE SOLAPAMIENTO:\n",
    "Entre las etiquetas `{overlap_start}` y `{overlap_end}` encontrarás texto que ya apareció en el bloque anterior:\n",
    "- No lo reescribas a menos que sea necesario para la continuidad.\n",
    "- Evita repetir ideas ya tratadas.\n",
    "\n",
    "Si encuentras alguna palabra, frase o fragmento que no entiendas o no puedas mejorar o contenido que no puedas procesar adecuadamente, déjalo tal cual pero márcalo entre etiquetas `<unprocessed>...texto...</unprocessed>` para que pueda revisarse manualmente más adelante.\n",
    "\n",
    "No inventes información nueva. Reescribe el texto original, pero mejora la redacción y elimina el contenido innecesario.\n",
    "El resultado debe ser un texto que parezca extraído de una publicación financiera profesional, sin rastro de su origen como material educativo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **4. 📁 Estructuración Final**\n",
    "- **Organización**: Un archivo .txt por lección\n",
    "- **Formato**: Texto plano listo para chunking con LangChain\n",
    "\n",
    "### 🔄 Pipeline RAG en este Notebook\n",
    "\n",
    "En este notebook completaremos el pipeline RAG:\n",
    "\n",
    "1. **📚 Document Loading**: Cargar archivos .txt con LangChain\n",
    "2. **✂️ Chunking**: Segmentación con RecursiveCharacterTextSplitter\n",
    "3. **🧠 Metadata Extraction**: Usar GPT-4o para extraer `level` y `main_topic` por documento\n",
    "4. **🗄️ Vector Store**: Crear índice semántico con FAISS y embeddings OpenAI\n",
    "5. **🔍 Retrieval**: Sistema de búsqueda semántica con filtros por metadatos\n",
    "6. **🤖 RAG Pipeline**: Cadena completa de pregunta-respuesta con GPT-4o-mini\n",
    "\n",
    "### 📂 Carga del Dataset Procesado\n",
    "\n",
    "Para ejecutar este notebook, necesitas subir los archivos de texto procesados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwAujE1qaCjA"
   },
   "source": [
    "### 📋 Instrucciones para subir el dataset\n",
    "\n",
    "**📦 Pasos para cargar los archivos de texto procesados:**\n",
    "1. 📁 Comprime los archivos .txt procesados en un archivo llamado `data.zip`\n",
    "2. 📤 Haz clic en el icono de carpeta 📁 en la barra lateral izquierda\n",
    "3. 🔄 Arrastra y suelta `data.zip` o usa \"Subir archivos\"\n",
    "4. ⏳ Espera a que se complete la subida\n",
    "5. ▶️ Ejecuta las celdas siguientes para:\n",
    "   - Extraer y cargar documentos con LangChain\n",
    "   - Aplicar chunking\n",
    "   - Extraer metadatos con GPT-4o (`level` y `main_topic`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8QP2I5CYzYM"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📤 Extraer y cargar archivos del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYbLW_YNYzYN"
   },
   "outputs": [],
   "source": [
    "if not SKIP_PROCESSING:\n",
    "    print(\"🔍 Buscando archivos del dataset...\")\n",
    "\n",
    "    if os.path.exists('data.zip'):\n",
    "        print(\"✅ Archivo data.zip encontrado. Extrayendo...\")\n",
    "        with zipfile.ZipFile('data.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall('.')\n",
    "        print(\"✅ Archivos extraídos correctamente\")\n",
    "    else:\n",
    "        print(\"❌ No se encontró data.zip\")\n",
    "        print(\"\\n📋 PASOS A SEGUIR:\")\n",
    "        print(\"1. Ve al panel de archivos (📁) en la barra lateral izquierda\")\n",
    "        print(\"2. Haz clic en 'Subir archivos' o arrastra data.zip\")\n",
    "        print(\"3. Espera a que se complete la subida\")\n",
    "        print(\"4. Vuelve a ejecutar esta celda\")\n",
    "\n",
    "    txt_files = glob.glob(\"data/*.txt\")\n",
    "    print(f\"\\n📁 Archivos de texto encontrados en 'data/': {len(txt_files)}\")\n",
    "\n",
    "    if len(txt_files) == 0:\n",
    "        print(\"❌ No se encontraron archivos de texto en la carpeta 'data'\")\n",
    "        print(\"🔄 Asegúrate de haber subido data.zip correctamente y que contenga la carpeta 'data' con archivos .txt\")\n",
    "    else:\n",
    "        print(f\"✅ Dataset encontrado: {len(txt_files)} lecciones de conocimiento financiero\")\n",
    "else:\n",
    "    print(\"⚡ SALTANDO: Ya tenemos chunks procesados desde Google Drive\")\n",
    "    txt_files = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6XAlEqMaP6u"
   },
   "source": [
    "## 📚 Document Loading con LangChain\n",
    "\n",
    "En esta sección cargaremos los documentos de texto utilizando LangChain y extraeremos metadatos desde los nombres de archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elnCxY0lYzYN"
   },
   "outputs": [],
   "source": [
    "if not SKIP_PROCESSING:\n",
    "    import re\n",
    "    from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "    from langchain.schema import Document\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    import tiktoken\n",
    "\n",
    "    def extract_metadata_from_filename(filename):\n",
    "        base_name = filename.replace('.txt', '')\n",
    "        parts = base_name.split('_')\n",
    "\n",
    "        if len(parts) >= 2:\n",
    "            try:\n",
    "                module = int(parts[0])\n",
    "                lesson = int(parts[1])\n",
    "\n",
    "                if 'bonus' in base_name.lower():\n",
    "                    content_type = \"bonus\"\n",
    "                elif 'conclusion' in base_name.lower():\n",
    "                    content_type = \"conclusión\"\n",
    "                elif lesson == 0:\n",
    "                    content_type = \"introducción\"\n",
    "                else:\n",
    "                    content_type = \"lección\"\n",
    "\n",
    "                return {\n",
    "                    'source_file': base_name,\n",
    "                    'module': module,\n",
    "                    'lesson': lesson,\n",
    "                    'content_type': content_type,\n",
    "                    'filename': filename\n",
    "                }\n",
    "            except (ValueError, IndexError):\n",
    "                pass\n",
    "\n",
    "        return {\n",
    "            'source_file': base_name,\n",
    "            'module': 0,\n",
    "            'lesson': 0,\n",
    "            'content_type': 'contenido',\n",
    "            'filename': filename\n",
    "        }\n",
    "\n",
    "    print(\"🔧 Funciones de extracción de metadatos configuradas\")\n",
    "else:\n",
    "    print(\"⚡ SALTANDO: Funciones de procesamiento no necesarias\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📖 Cargar documentos con LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rubPWSMYzYN"
   },
   "outputs": [],
   "source": [
    "if not SKIP_PROCESSING:\n",
    "    print(\"📚 Cargando documentos con LangChain...\")\n",
    "\n",
    "    if len(txt_files) > 0:\n",
    "        raw_documents = []\n",
    "        for txt_file in txt_files:\n",
    "            try:\n",
    "                loader = TextLoader(txt_file, encoding='utf-8')\n",
    "                doc = loader.load()[0]\n",
    "                raw_documents.append(doc)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error cargando {txt_file}: {e}\")\n",
    "\n",
    "        print(f\"✅ Cargados {len(raw_documents)} documentos base\")\n",
    "\n",
    "        enriched_documents = []\n",
    "        for doc in raw_documents:\n",
    "            filename = os.path.basename(doc.metadata['source'])\n",
    "            basic_metadata = extract_metadata_from_filename(filename)\n",
    "            doc.metadata.update(basic_metadata)\n",
    "            enriched_documents.append(doc)\n",
    "\n",
    "        print(f\"✅ Documentos enriquecidos con metadatos básicos\")\n",
    "\n",
    "        if enriched_documents:\n",
    "            sample_doc = enriched_documents[0]\n",
    "            print(f\"\\n📋 Ejemplo de metadatos básicos:\")\n",
    "            print(f\"📁 Archivo: {sample_doc.metadata['filename']}\")\n",
    "            print(f\"📚 Módulo: {sample_doc.metadata['module']}\")\n",
    "            print(f\"📖 Lección: {sample_doc.metadata['lesson']}\")\n",
    "            print(f\"🔖 Tipo: {sample_doc.metadata['content_type']}\")\n",
    "            content_preview = sample_doc.page_content[:200] + \"...\" if len(sample_doc.page_content) > 200 else sample_doc.page_content\n",
    "            print(f\"\\n📝 Vista previa del contenido:\")\n",
    "            print(f\"'{content_preview}'\")\n",
    "\n",
    "            print(f\"\\n💡 Nota: Los metadatos 'level' y 'main_topic' se extraerán con GPT-4o automáticamente\")\n",
    "    else:\n",
    "        print(\"❌ No hay documentos para cargar\")\n",
    "else:\n",
    "    print(\"⚡ SALTANDO: Carga de documentos no necesaria\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSITPxIeaYzf"
   },
   "source": [
    "## ✂️ Chunking Strategy con LangChain\n",
    "\n",
    "Implementaremos una estrategia de chunking utilizando el `RecursiveCharacterTextSplitter` de LangChain, optimizado para contenido financiero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✂️ Configurar Text Splitter optimizado para contenido financiero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8lUD1UPYzYN"
   },
   "outputs": [],
   "source": [
    "if not SKIP_PROCESSING:\n",
    "    print(\"🔧 Configurando estrategia de chunking...\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=150,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \"],\n",
    "        add_start_index=True\n",
    "    )\n",
    "\n",
    "    print(\"✅ Text Splitter configurado:\")\n",
    "    print(f\"   📏 Chunk size: {text_splitter._chunk_size}\")\n",
    "    print(f\"   🔗 Overlap: {text_splitter._chunk_overlap}\")\n",
    "    print(f\"   📊 Separadores: {len(text_splitter._separators)} niveles\")\n",
    "else:\n",
    "    print(\"⚡ SALTANDO: Configuración de chunking no necesaria\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔄 Aplicar chunking a los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQ69ZFLaYzYN"
   },
   "outputs": [],
   "source": [
    "if not SKIP_PROCESSING:\n",
    "    print(\"✂️ Aplicando chunking a los documentos...\")\n",
    "\n",
    "    if 'enriched_documents' in locals() and enriched_documents:\n",
    "        chunks = text_splitter.split_documents(enriched_documents)\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk.metadata['chunk_id'] = i\n",
    "            chunk.metadata['chunk_length'] = len(chunk.page_content)\n",
    "\n",
    "            try:\n",
    "                encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "                chunk.metadata['chunk_tokens'] = len(encoding.encode(chunk.page_content))\n",
    "            except:\n",
    "                chunk.metadata['chunk_tokens'] = len(chunk.page_content) // 4\n",
    "\n",
    "        print(f\"✅ Chunking completado:\")\n",
    "        print(f\"   📚 Documentos originales: {len(enriched_documents)}\")\n",
    "        print(f\"   🧩 Chunks generados: {len(chunks)}\")\n",
    "        print(f\"   📊 Promedio chunks por documento: {len(chunks) / len(enriched_documents):.1f}\")\n",
    "\n",
    "        chunk_lengths = [chunk.metadata['chunk_length'] for chunk in chunks]\n",
    "        chunk_tokens = [chunk.metadata['chunk_tokens'] for chunk in chunks]\n",
    "\n",
    "        print(f\"\\n📈 Estadísticas de chunks:\")\n",
    "        print(f\"   📏 Longitud promedio: {sum(chunk_lengths) / len(chunk_lengths):.0f} caracteres\")\n",
    "        print(f\"   🔤 Tokens promedio: {sum(chunk_tokens) / len(chunk_tokens):.0f} tokens\")\n",
    "        print(f\"   📊 Longitud mín/máx: {min(chunk_lengths)}/{max(chunk_lengths)} caracteres\")\n",
    "        print(f\"   🎯 Tokens mín/máx: {min(chunk_tokens)}/{max(chunk_tokens)} tokens\")\n",
    "\n",
    "        if chunks:\n",
    "            sample_chunk = chunks[0]\n",
    "            print(sample_chunk)\n",
    "            print(f\"\\n📋 Ejemplo de chunk generado:\")\n",
    "            print(f\"   🆔 Chunk ID: {sample_chunk.metadata['chunk_id']}\")\n",
    "            print(f\"   📁 Documento origen: {sample_chunk.metadata['filename']}\")\n",
    "            print(f\"   📏 Longitud: {sample_chunk.metadata['chunk_length']} caracteres\")\n",
    "            print(f\"   🔤 Tokens: {sample_chunk.metadata['chunk_tokens']} tokens\")\n",
    "            preview = sample_chunk.page_content[:300] + \"...\" if len(sample_chunk.page_content) > 300 else sample_chunk.page_content\n",
    "            print(f\"   📝 Contenido: '{preview}'\")\n",
    "\n",
    "    else:\n",
    "        print(\"❌ No hay documentos cargados para hacer chunking\")\n",
    "else:\n",
    "    print(\"⚡ SALTANDO: Chunking no necesario\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🤖 Extracción de Metadatos con GPT-4o\n",
    "\n",
    "Esta sección utiliza **GPT-4o** para analizar automáticamente el contenido de cada documento y extraer metadatos clave:\n",
    "\n",
    "- **📋 main_topic**: Identifica el tema principal de cada lección (ej: \"Ventajas de invertir en dividendos\")\n",
    "- **📊 level**: Determina el nivel de dificultad del contenido (básico, intermedio, avanzado)\n",
    "\n",
    "Estos metadatos enriquecen los chunks y permiten filtros más precisos durante la recuperación de información en el sistema RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QvNgZ2D6YzYO"
   },
   "outputs": [],
   "source": [
    "if not SKIP_PROCESSING:\n",
    "    from openai import OpenAI\n",
    "    import tiktoken\n",
    "    import json as json_lib\n",
    "    from collections import defaultdict\n",
    "\n",
    "    client = OpenAI()\n",
    "\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "    def count_tokens(text):\n",
    "        return len(encoding.encode(text))\n",
    "\n",
    "    def truncate_text_if_needed(text, max_tokens=8000):\n",
    "        tokens = encoding.encode(text)\n",
    "        if len(tokens) <= max_tokens:\n",
    "            return text, len(tokens)\n",
    "\n",
    "        truncated_tokens = tokens[:max_tokens]\n",
    "        truncated_text = encoding.decode(truncated_tokens)\n",
    "        print(f\"⚠️  Texto truncado de {len(tokens)} a {len(truncated_tokens)} tokens\")\n",
    "        return truncated_text, len(truncated_tokens)\n",
    "\n",
    "    def analyze_content_with_gpt4o(text):\n",
    "        final_text, num_tokens = truncate_text_if_needed(text)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Eres un asistente experto en educación financiera. Se te da el contenido de una lección sobre inversión en dividendos.\n",
    "\n",
    "        Tu tarea es:\n",
    "        1. Identificar el tema principal (una frase corta, como \"Ventajas de invertir en dividendos\").\n",
    "        2. Estimar el nivel de dificultad del contenido (básico, intermedio o avanzado).\n",
    "\n",
    "        Contenido de la lección ({num_tokens} tokens):\n",
    "        \\\"\\\"\\\"\\n{final_text}\\n\\\"\\\"\\\"\n",
    "\n",
    "        RESPONDE ÚNICAMENTE CON EL JSON, SIN MARKDOWN, SIN EXPLICACIONES, SIN TEXTO ADICIONAL:\n",
    "\n",
    "        {{\n",
    "            \"main_topic\": \"...\",\n",
    "            \"level\": \"básico | intermedio | avanzado\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.1,\n",
    "                    max_tokens=150\n",
    "                )\n",
    "\n",
    "                response_text = response.choices[0].message.content.strip()\n",
    "                \n",
    "                try:\n",
    "                    return json_lib.loads(response_text)\n",
    "                except json_lib.JSONDecodeError:\n",
    "                    import re\n",
    "                    json_match = re.search(r'\\{[^}]+\\}', response_text)\n",
    "                    if json_match:\n",
    "                        try:\n",
    "                            return json_lib.loads(json_match.group())\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    if attempt < max_retries - 1:\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "        return {\"main_topic\": \"Contenido financiero\", \"level\": \"intermedio\"}\n",
    "\n",
    "    print(\"🔧 Funciones de extracción de metadatos con GPT-4o configuradas\")\n",
    "else:\n",
    "    print(\"⚡ SALTANDO: Funciones de GPT-4o no necesarias\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧠 Extraer metadatos por documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClZFt6z7YzYO"
   },
   "outputs": [],
   "source": [
    "if not SKIP_PROCESSING:\n",
    "    print(\"🤖 Extrayendo metadatos con GPT-4o...\")\n",
    "\n",
    "    if 'chunks' in locals() and chunks:\n",
    "        docs_by_source = defaultdict(list)\n",
    "        for chunk in chunks:\n",
    "            source_file = chunk.metadata['source_file']\n",
    "            docs_by_source[source_file].append(chunk)\n",
    "\n",
    "        print(f\"📚 Procesando {len(docs_by_source)} documentos únicos...\")\n",
    "\n",
    "        metadata_cache = {}\n",
    "\n",
    "        for i, (source_file, doc_chunks) in enumerate(docs_by_source.items()):\n",
    "            print(f\"\\n🔄 Procesando ({i+1}/{len(docs_by_source)}): {source_file}\")\n",
    "            full_content = \" \".join([chunk.page_content for chunk in doc_chunks])\n",
    "\n",
    "            try:\n",
    "                metadata = analyze_content_with_gpt4o(full_content)\n",
    "                metadata_cache[source_file] = metadata\n",
    "\n",
    "                print(f\"   ✅ Tema: {metadata['main_topic']}\")\n",
    "                print(f\"   📊 Nivel: {metadata['level']}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Error: {e}\")\n",
    "                metadata_cache[source_file] = {\n",
    "                    \"main_topic\": \"Contenido financiero\",\n",
    "                    \"level\": \"intermedio\"\n",
    "                }\n",
    "\n",
    "        print(f\"\\n🔄 Aplicando metadatos a {len(chunks)} chunks...\")\n",
    "\n",
    "        for chunk in chunks:\n",
    "            source_file = chunk.metadata['source_file']\n",
    "            if source_file in metadata_cache:\n",
    "                chunk.metadata['main_topic'] = metadata_cache[source_file]['main_topic']\n",
    "                chunk.metadata['level'] = metadata_cache[source_file]['level']\n",
    "\n",
    "        print(f\"✅ Metadatos aplicados a todos los chunks\")\n",
    "\n",
    "        if chunks:\n",
    "            sample_chunk = chunks[0]\n",
    "            print(f\"\\n📋 Ejemplo de chunk con metadatos completos:\")\n",
    "            print(f\"   🆔 Chunk ID: {sample_chunk.metadata['chunk_id']}\")\n",
    "            print(f\"   📁 Archivo: {sample_chunk.metadata['filename']}\")\n",
    "            print(f\"   📚 Módulo: {sample_chunk.metadata['module']}\")\n",
    "            print(f\"   📖 Lección: {sample_chunk.metadata['lesson']}\")\n",
    "            print(f\"   🎯 Tema: {sample_chunk.metadata['main_topic']}\")\n",
    "            print(f\"   📊 Nivel: {sample_chunk.metadata['level']}\")\n",
    "            print(f\"   🔖 Tipo: {sample_chunk.metadata['content_type']}\")\n",
    "            print(f\"   🔤 Tokens: {sample_chunk.metadata['chunk_tokens']}\")\n",
    "\n",
    "        print(f\"\\n💾 Guardando chunks procesados en Google Drive...\")\n",
    "        try:\n",
    "            with open(CHUNKS_PATH, 'wb') as f:\n",
    "                pickle.dump(chunks, f)\n",
    "            print(f\"✅ {len(chunks)} chunks guardados exitosamente en Google Drive\")\n",
    "            print(f\"📁 Ruta: {CHUNKS_PATH}\")\n",
    "            print(f\"💡 En futuras ejecuciones se cargarán automáticamente desde aquí\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error guardando chunks: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(\"❌ No hay chunks disponibles para extraer metadatos\")\n",
    "else:\n",
    "    print(\"⚡ SALTANDO: Extracción de metadatos no necesaria\")\n",
    "\n",
    "print(\"✅ ¡Dataset listo para implementar el sistema RAG completo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 🗄️ Vector Store con FAISS\n",
    "\n",
    "Implementaremos el vector store usando FAISS para búsqueda semántica sobre nuestros chunks procesados con metadatos completos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💾 Verificación de Vector Store existente en Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 Verificando vector store existente en Google Drive...\")\n",
    "\n",
    "VECTORSTORE_PATH = '/content/drive/MyDrive/RAG_Dividendos/vectorstore_faiss'\n",
    "VECTORSTORE_INDEX_PATH = '/content/drive/MyDrive/RAG_Dividendos/vectorstore_faiss/index.faiss'\n",
    "VECTORSTORE_PKL_PATH = '/content/drive/MyDrive/RAG_Dividendos/vectorstore_faiss/index.pkl'\n",
    "\n",
    "vectorstore_exists = os.path.exists(VECTORSTORE_INDEX_PATH) and os.path.exists(VECTORSTORE_PKL_PATH)\n",
    "\n",
    "if vectorstore_exists:\n",
    "    print(\"✅ ¡Vector store encontrado en Google Drive!\")\n",
    "    print(f\"📁 Índice FAISS: {VECTORSTORE_INDEX_PATH}\")\n",
    "    print(f\"📁 Metadatos: {VECTORSTORE_PKL_PATH}\")\n",
    "    \n",
    "    try:\n",
    "        from langchain_community.vectorstores import FAISS\n",
    "        from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "        \n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "        \n",
    "        print(\"📥 Cargando vector store desde Google Drive...\")\n",
    "        vectorstore = FAISS.load_local(\n",
    "            VECTORSTORE_PATH, \n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        \n",
    "        print(f\"🎉 ¡Vector store cargado exitosamente!\")\n",
    "        print(f\"   📊 Vectores en el índice: {vectorstore.index.ntotal}\")\n",
    "        print(f\"   📐 Dimensiones: {vectorstore.index.d}\")\n",
    "        \n",
    "        print(f\"\\n⚡ SALTANDO CREACIÓN DE VECTOR STORE - usando existente\")\n",
    "        SKIP_VECTORSTORE = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error cargando vector store: {e}\")\n",
    "        print(f\"🔄 Se creará un nuevo vector store\")\n",
    "        SKIP_VECTORSTORE = False\n",
    "        vectorstore = None\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No se encontró vector store en Google Drive\")\n",
    "    print(f\"🔄 Se creará un nuevo vector store desde cero\")\n",
    "    SKIP_VECTORSTORE = False\n",
    "    vectorstore = None\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "if SKIP_VECTORSTORE:\n",
    "    print(\"🚀 MODO RÁPIDO: Usando vector store existente\")\n",
    "    print(\"💡 Listo para búsquedas semánticas\")\n",
    "else:\n",
    "    print(\"🔄 MODO CREACIÓN: Generando vector store desde cero\")\n",
    "    print(\"💡 Se crearán embeddings para todos los chunks\")\n",
    "print(f\"{'='*50}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧠 Creación del Vector Store con FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_VECTORSTORE:\n",
    "    print(\"🔧 Creando vector store con FAISS...\")\n",
    "    \n",
    "    if 'chunks' in locals() and chunks and len(chunks) > 0:\n",
    "        print(f\"📊 Procesando {len(chunks)} chunks para crear embeddings...\")\n",
    "        \n",
    "        from langchain_community.vectorstores import FAISS\n",
    "        from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "        import time\n",
    "        \n",
    "        print(\"🔧 Inicializando OpenAI Embeddings (text-embedding-3-large)...\")\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "        \n",
    "        print(\"⏳ Creando embeddings y construyendo índice FAISS...\")\n",
    "        print(\"💡 Este proceso puede tomar varios minutos dependiendo del número de chunks...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            vectorstore = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            processing_time = end_time - start_time\n",
    "            \n",
    "            print(f\"✅ Vector store creado exitosamente!\")\n",
    "            print(f\"   ⏱️ Tiempo de procesamiento: {processing_time:.1f} segundos\")\n",
    "            print(f\"   📊 Vectores en el índice: {vectorstore.index.ntotal}\")\n",
    "            print(f\"   📐 Dimensiones del embedding: {vectorstore.index.d}\")\n",
    "            print(f\"   🧩 Chunks procesados: {len(chunks)}\")\n",
    "            \n",
    "            if chunks:\n",
    "                sample_doc = vectorstore.similarity_search(\"dividendos\", k=1)[0]\n",
    "                print(f\"\\n📋 Verificación de metadatos en vector store:\")\n",
    "                print(f\"   📁 Archivo: {sample_doc.metadata.get('filename', 'N/A')}\")\n",
    "                print(f\"   🎯 Tema: {sample_doc.metadata.get('main_topic', 'N/A')}\")\n",
    "                print(f\"   📊 Nivel: {sample_doc.metadata.get('level', 'N/A')}\")\n",
    "                print(f\"   📚 Módulo: {sample_doc.metadata.get('module', 'N/A')}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error creando vector store: {e}\")\n",
    "            print(\"🔄 Verifica tu API key de OpenAI y conexión a internet\")\n",
    "            vectorstore = None\n",
    "            \n",
    "    else:\n",
    "        print(\"❌ No hay chunks disponibles para crear el vector store\")\n",
    "        print(\"🔄 Asegúrate de haber ejecutado las celdas de procesamiento de chunks\")\n",
    "        vectorstore = None\n",
    "        \n",
    "else:\n",
    "    print(\"⚡ SALTANDO: Vector store ya cargado desde Google Drive\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💾 Persistencia del Vector Store en Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_VECTORSTORE and vectorstore is not None:\n",
    "    print(\"💾 Guardando vector store en Google Drive...\")\n",
    "    \n",
    "    try:\n",
    "        vectorstore.save_local(VECTORSTORE_PATH)\n",
    "        \n",
    "        print(f\"✅ Vector store guardado exitosamente en Google Drive!\")\n",
    "        print(f\"📁 Ruta base: {VECTORSTORE_PATH}\")\n",
    "        print(f\"📁 Índice FAISS: {VECTORSTORE_INDEX_PATH}\")\n",
    "        print(f\"📁 Metadatos: {VECTORSTORE_PKL_PATH}\")\n",
    "        print(f\"💡 En futuras ejecuciones se cargará automáticamente desde aquí\")\n",
    "        \n",
    "        if os.path.exists(VECTORSTORE_INDEX_PATH) and os.path.exists(VECTORSTORE_PKL_PATH):\n",
    "            index_size = os.path.getsize(VECTORSTORE_INDEX_PATH) / (1024*1024)\n",
    "            pkl_size = os.path.getsize(VECTORSTORE_PKL_PATH) / (1024*1024)\n",
    "            print(f\"📊 Tamaño del índice: {index_size:.1f} MB\")\n",
    "            print(f\"📊 Tamaño de metadatos: {pkl_size:.1f} MB\")\n",
    "        else:\n",
    "            print(\"⚠️ Advertencia: No se pudieron verificar todos los archivos guardados\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error guardando vector store: {e}\")\n",
    "        print(\"⚠️ El vector store se ha creado pero no se pudo guardar en Google Drive\")\n",
    "        print(\"💡 Podrás usarlo en esta sesión, pero se perderá al reiniciar\")\n",
    "        \n",
    "elif SKIP_VECTORSTORE:\n",
    "    print(\"⚡ SALTANDO: Vector store ya existía en Google Drive\")\n",
    "else:\n",
    "    print(\"❌ No hay vector store para guardar (no se creó correctamente)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔍 Configuración del Retriever para búsquedas semánticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔧 Configurando retriever para búsquedas semánticas...\")\n",
    "\n",
    "if vectorstore is not None:\n",
    "    # 1. Retriever básico por similitud\n",
    "    basic_retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5} \n",
    "    )\n",
    "    \n",
    "    # 2. Retriever con threshold de similitud (más estricto)\n",
    "    similarity_retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={\"k\": 10, \"score_threshold\": 0.5}\n",
    "    )\n",
    "    \n",
    "    # 3. Retriever con MMR (Maximum Marginal Relevance) para diversidad\n",
    "    mmr_retriever = vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\n",
    "            \"k\": 5,\n",
    "            \"fetch_k\": 20,              # Buscar entre los top 20\n",
    "            \"lambda_mult\": 0.7          # Balance relevancia vs diversidad\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Retrievers configurados:\")\n",
    "    print(\"   🔍 basic_retriever: Top 5 chunks más similares\")\n",
    "    print(\"   🎯 similarity_retriever: Chunks con similitud > 0.7\")\n",
    "    print(\"   🌈 mmr_retriever: Top 5 con diversidad (MMR)\")\n",
    "    \n",
    "    def search_with_filters(query, level=None, module=None, content_type=None, k=5):\n",
    "        filter_dict = {}\n",
    "        \n",
    "        if level:\n",
    "            filter_dict['level'] = level\n",
    "        if module:\n",
    "            filter_dict['module'] = module\n",
    "        if content_type:\n",
    "            filter_dict['content_type'] = content_type\n",
    "        \n",
    "        if filter_dict:\n",
    "            results = vectorstore.similarity_search_with_score(query, k=k, filter=filter_dict)\n",
    "        else:\n",
    "            results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    print(\"✅ Función de búsqueda con filtros configurada\")\n",
    "    print(\"💡 Usa search_with_filters(query, level='intermedio', module=2, k=3)\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No se puede configurar retriever: vector store no disponible\")\n",
    "    print(\"🔄 Asegúrate de haber creado o cargado el vector store correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 Pruebas del Vector Store con consultas sobre inversión en dividendos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧪 Probando el vector store con consultas de ejemplo...\")\n",
    "\n",
    "if vectorstore is not None:\n",
    "    test_queries = [\n",
    "        \"¿Qué son los dividendos y por qué las empresas los pagan?\",\n",
    "        \"¿Cuáles son las ventajas de invertir en dividendos?\",\n",
    "        \"¿Cómo seleccionar acciones que paguen dividendos?\",\n",
    "        \"¿Qué es la rentabilidad por dividendo?\",\n",
    "        \"¿Cuáles son los riesgos de la inversión en dividendos?\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"📋 Ejecutando {len(test_queries)} consultas de prueba...\\n\")\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"🔍 Consulta {i}: '{query}'\")\n",
    "        \n",
    "        try:\n",
    "            results = vectorstore.similarity_search_with_score(query, k=3)\n",
    "            \n",
    "            print(f\"   📊 Encontrados {len(results)} chunks relevantes:\")\n",
    "            \n",
    "            for j, (doc, score) in enumerate(results, 1):\n",
    "                print(f\"   {j}. 📄 {doc.metadata.get('filename', 'N/A')[:50]}...\")\n",
    "                print(f\"      🎯 Tema: {doc.metadata.get('main_topic', 'N/A')}\")\n",
    "                print(f\"      📊 Nivel: {doc.metadata.get('level', 'N/A')}\")\n",
    "                print(f\"      🔢 Score: {score:.3f}\")\n",
    "                print(f\"      📝 Contenido: {doc.page_content[:100]}...\")\n",
    "                print()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error en búsqueda: {e}\")\n",
    "            \n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    print(\"\\n🎯 Prueba con filtros por metadatos:\")\n",
    "    print(\"🔍 Búsqueda: 'dividendos' solo en contenido intermedio...\")\n",
    "    \n",
    "    try:\n",
    "        filtered_results = search_with_filters(\"dividendos\", level=\"intermedio\", k=3)\n",
    "        \n",
    "        print(f\"   📊 Encontrados {len(filtered_results)} chunks intermedios:\")\n",
    "        for i, (doc, score) in enumerate(filtered_results, 1):\n",
    "            print(f\"   {i}. 📄 {doc.metadata.get('filename', 'N/A')}\")\n",
    "            print(f\"      📊 Nivel: {doc.metadata.get('level', 'N/A')}\")\n",
    "            print(f\"      📚 Módulo: {doc.metadata.get('module', 'N/A')}\")\n",
    "            print(f\"      🔢 Score: {score:.3f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error en búsqueda filtrada: {e}\")\n",
    "    \n",
    "    print(\"\\n✅ ¡Vector store funcionando correctamente!\")\n",
    "    print(\"💡 Listo para implementar el pipeline RAG completo\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No se pueden ejecutar pruebas: vector store no disponible\")\n",
    "    print(\"🔄 Asegúrate de haber creado o cargado el vector store correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 🤖 Pipeline RAG Completo\n",
    "\n",
    "Implementaremos el sistema completo de pregunta-respuesta combinando retrieval semántico con generación de respuestas usando GPT-4o-mini.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔧 Configuración del LLM y Prompt Template especializado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔧 Configurando LLM y prompt template para finanzas...\")\n",
    "\n",
    "if vectorstore is not None:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "    from langchain_core.runnables import RunnablePassthrough\n",
    "    \n",
    "    base_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1, max_tokens=1000)\n",
    "    \n",
    "    financial_prompt_template = \"\"\"\n",
    "    Eres un asistente experto en inversión en dividendos y análisis financiero. Tu objetivo es proporcionar respuestas precisas, educativas y prácticas basándose únicamente en el contexto proporcionado.\n",
    "\n",
    "    INSTRUCCIONES:\n",
    "    1. Responde ÚNICAMENTE basándote en el contexto proporcionado\n",
    "    2. Si la información no está en el contexto, responde de forma BREVE usando una de estas variaciones:\n",
    "        - \"No tengo información sobre [tema] en mi base de conocimiento financiero.\"\n",
    "        - \"Esta consulta está fuera de mi especialización en dividendos.\"\n",
    "        - \"No encuentro información sobre [tema] en el contexto proporcionado.\"\n",
    "        - \"Mi conocimiento se limita a inversión en dividendos.\"\n",
    "        - \"No puedo ayudar con [tema], solo con temas financieros.\"\n",
    "    3. Usa un lenguaje claro y profesional apropiado para inversores\n",
    "    4. Incluye ejemplos prácticos cuando sea relevante\n",
    "    5. Estructura tu respuesta de forma clara y organizada\n",
    "    6. Si mencionas conceptos técnicos, explícalos brevemente\n",
    "    \n",
    "    CONTEXTO RELEVANTE:\n",
    "    {context}\n",
    "\n",
    "    PREGUNTA DEL USUARIO:\n",
    "    {question}\n",
    "\n",
    "    RESPUESTA EXPERTA:\n",
    "    \"\"\"\n",
    "\n",
    "    financial_prompt = PromptTemplate(template=financial_prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    \n",
    "    print(\"✅ LLM configurado:\")\n",
    "    print(f\"   🤖 Modelo: {base_model.model_name}\")\n",
    "    print(f\"   🌡️ Temperature: {base_model.temperature}\")\n",
    "    print(f\"   📝 Max tokens: {base_model.max_tokens}\")\n",
    "    \n",
    "    print(\"✅ Prompt template especializado en finanzas creado\")\n",
    "    print(\"💡 Optimizado para respuestas educativas y precisas sobre inversión en dividendos\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No se puede configurar el pipeline RAG: vector store no disponible\")\n",
    "    print(\"🔄 Asegúrate de haber creado o cargado el vector store correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔗 Creación de la cadena RAG (Retrieval + Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔗 Creando cadena RAG completa...\")\n",
    "\n",
    "if vectorstore is not None and 'base_model' in locals():\n",
    "    rag_chain = (\n",
    "        {\"context\": basic_retriever, \"question\": RunnablePassthrough()}\n",
    "        | financial_prompt\n",
    "        | base_model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    rag_chain_similarity = (\n",
    "        {\"context\": similarity_retriever, \"question\": RunnablePassthrough()}\n",
    "        | financial_prompt\n",
    "        | base_model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    rag_chain_mmr = (\n",
    "        {\"context\": mmr_retriever, \"question\": RunnablePassthrough()}\n",
    "        | financial_prompt\n",
    "        | base_model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Cadenas RAG creadas exitosamente:\")\n",
    "    print(\"   🔗 rag_chain: Cadena principal (retriever básico)\")\n",
    "    print(\"   🎯 rag_chain_similarity: Con threshold de similitud\")\n",
    "    print(\"   🌈 rag_chain_mmr: Con diversidad (MMR)\")\n",
    "    \n",
    "    print(\"\\n💡 Características de las cadenas:\")\n",
    "    print(\"   🚀 Método: LCEL (LangChain Expression Language)\")\n",
    "    print(\"   ⚡ Más rápido y eficiente que RetrievalQA\")\n",
    "    print(\"   🎯 Prompt: Especializado en finanzas\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No se pueden crear las cadenas RAG\")\n",
    "    if vectorstore is None:\n",
    "        print(\"   - Vector store no disponible\")\n",
    "    if 'base_model' not in locals():\n",
    "        print(\"   - LLM no configurado\")\n",
    "    print(\"🔄 Asegúrate de haber ejecutado las celdas anteriores correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎯 Funciones helper para consultas especializadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎯 Creando funciones helper para consultas...\")\n",
    "\n",
    "if 'rag_chain' in locals():\n",
    "    \n",
    "    def ask_dividends_expert(question, retriever_type=\"basic\", show_sources=True):\n",
    "        if retriever_type == \"similarity\":\n",
    "            chain = rag_chain_similarity\n",
    "            retriever = similarity_retriever\n",
    "        elif retriever_type == \"mmr\":\n",
    "            chain = rag_chain_mmr\n",
    "            retriever = mmr_retriever\n",
    "        else:\n",
    "            chain = rag_chain\n",
    "            retriever = basic_retriever\n",
    "        \n",
    "        try:\n",
    "            answer = chain.invoke(question)\n",
    "\n",
    "            sources = []\n",
    "            if show_sources:\n",
    "                sources = retriever.invoke(question)\n",
    "            \n",
    "            response = {\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"sources\": sources,\n",
    "                \"retriever_type\": retriever_type\n",
    "            }\n",
    "            \n",
    "            if show_sources:\n",
    "                print(f\"🤖 **RESPUESTA DEL EXPERTO:**\")\n",
    "                print(f\"{answer}\")\n",
    "                print(f\"\\n📚 **FUENTES CONSULTADAS ({len(response['sources'])} documentos):**\")\n",
    "                \n",
    "                for i, doc in enumerate(response['sources'], 1):\n",
    "                    print(f\"\\n{i}. 📄 **{doc.metadata.get('filename', 'N/A')}**\")\n",
    "                    print(f\"   🎯 Tema: {doc.metadata.get('main_topic', 'N/A')}\")\n",
    "                    print(f\"   📊 Nivel: {doc.metadata.get('level', 'N/A')}\")\n",
    "                    print(f\"   📚 Módulo: {doc.metadata.get('module', 'N/A')}\")\n",
    "                    print(f\"   📝 Extracto: {doc.page_content[:150]}...\")\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error ejecutando consulta: {e}\")\n",
    "            return None\n",
    "    \n",
    "    print(\"✅ Función helper creada:\")\n",
    "    print(\"   🤖 ask_dividends_expert(): Consulta principal\")\n",
    "    \n",
    "    print(\"\\n💡 Ejemplo de uso:\")\n",
    "    print('   ask_dividends_expert(\"¿Qué son los dividendos?\")')\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No se puede crear la función helper: cadena RAG no disponible\")\n",
    "    print(\"🔄 Asegúrate de haber ejecutado las celdas anteriores correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 Pruebas completas del Sistema RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧪 Probando el sistema RAG completo...\")\n",
    "\n",
    "if 'ask_dividends_expert' in locals():\n",
    "    test_questions = [\n",
    "        {\n",
    "            \"question\": \"¿Qué son los dividendos?\",\n",
    "            \"description\": \"Pregunta básica sobre conceptos fundamentales\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"¿Cuáles son las principales ventajas de invertir en dividendos?\",\n",
    "            \"description\": \"Pregunta sobre beneficios de la estrategia\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"¿Cómo puedo evaluar si una empresa paga dividendos sostenibles?\",\n",
    "            \"description\": \"Pregunta práctica sobre análisis\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"¿Cuáles son los principales ratios financieros para evaluar empresas que reparten dividendos?\",\n",
    "            \"description\": \"Pregunta sobre ratios financieros\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"¿Qué riesgos debo considerar al invertir en dividendos?\",\n",
    "            \"description\": \"Pregunta sobre gestión de riesgo\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"🎯 Ejecutando {len(test_questions)} pruebas del sistema RAG...\\n\")\n",
    "    \n",
    "    for i, test in enumerate(test_questions, 1):\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"🔍 PRUEBA {i}: {test['description']}\")\n",
    "        print(f\"❓ PREGUNTA: {test['question']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            result = ask_dividends_expert(\n",
    "                test['question'], \n",
    "                retriever_type=\"similarity\",\n",
    "                show_sources=True\n",
    "            )\n",
    "            \n",
    "            if result:\n",
    "                print(f\"\\n✅ Consulta ejecutada exitosamente\")\n",
    "                print(f\"📊 Fuentes utilizadas: {len(result['sources'])}\")\n",
    "            else:\n",
    "                print(f\"\\n❌ Error en la consulta\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error ejecutando prueba {i}: {e}\")\n",
    "        \n",
    "        print(f\"\\n{'-'*80}\\n\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"🎉 ¡SISTEMA RAG COMPLETAMENTE FUNCIONAL!\")\n",
    "    print(\"💡 El asistente de inversión en dividendos está listo para usar\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No se pueden ejecutar pruebas: funciones RAG no disponibles\")\n",
    "    print(\"🔄 Asegúrate de haber ejecutado todas las celdas anteriores correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 Verificación de que el sistema usa SOLO el RAG (no conocimiento externo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧪 Verificando que el sistema usa ÚNICAMENTE el RAG...\")\n",
    "\n",
    "if 'ask_dividends_expert' in locals():\n",
    "    non_rag_questions = [\n",
    "        {\n",
    "            \"question\": \"¿Quién es Lionel Messi?\",\n",
    "            \"expected\": \"No debería saber nada sobre fútbol\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"¿Cómo funciona la inteligencia artificial?\",\n",
    "            \"expected\": \"No debería tener info sobre IA general\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"¿Cuál es la capital de Francia?\",\n",
    "            \"expected\": \"No debería saber geografía\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"¿Qué es Python en programación?\",\n",
    "            \"expected\": \"No debería saber sobre programación\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"¿Cómo se hace una paella?\",\n",
    "            \"expected\": \"No debería saber cocina\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"🎯 Ejecutando {len(non_rag_questions)} preguntas FUERA del dominio financiero...\\n\")\n",
    "    \n",
    "    for i, test in enumerate(non_rag_questions, 1):\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"🔍 PRUEBA ANTI-RAG {i}: {test['expected']}\")\n",
    "        print(f\"❓ PREGUNTA: {test['question']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            result = ask_dividends_expert(\n",
    "                test['question'], \n",
    "                retriever_type=\"similarity\",\n",
    "                show_sources=True\n",
    "            )\n",
    "            \n",
    "            if result:\n",
    "                print(f\"\\n✅ Consulta ejecutada exitosamente\")\n",
    "                print(f\"📊 Fuentes utilizadas: {len(result['sources'])}\")\n",
    "            else:\n",
    "                print(f\"\\n❌ Error en la consulta\")\n",
    "            \n",
    "            no_info_indicators = [\n",
    "                \"no tengo información\",\n",
    "                \"no está en el contexto\",\n",
    "                \"no puedo responder\",\n",
    "                \"información no disponible\",\n",
    "                \"no se encuentra\",\n",
    "                \"no hay información\"\n",
    "            ]\n",
    "            \n",
    "            has_no_info = any(indicator in result['answer'].lower() for indicator in no_info_indicators)\n",
    "            \n",
    "            if has_no_info:\n",
    "                print(f\"✅ **CORRECTO**: El sistema indica que no tiene la información\")\n",
    "            else:\n",
    "                print(f\"⚠️ **ATENCIÓN**: El sistema podría estar usando conocimiento externo\")\n",
    "                print(f\"💡 Revisar si el prompt está funcionando correctamente\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error ejecutando prueba anti-RAG {i}: {e}\")\n",
    "        \n",
    "        print(f\"\\n{'-'*80}\\n\")\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"🎯 **RESUMEN DE VERIFICACIÓN:**\")\n",
    "    print(\"✅ Si todas las respuestas indican 'no tengo información' → RAG funciona correctamente\")\n",
    "    print(\"⚠️ Si alguna respuesta da información externa → Revisar configuración del prompt\")\n",
    "    print(\"💡 Un buen sistema RAG debe decir 'no sé' cuando no tiene la información en su base de conocimiento\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No se puede ejecutar la verificación: función RAG no disponible\")\n",
    "    print(\"🔄 Asegúrate de haber ejecutado todas las celdas anteriores correctamente\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhFrJ-CHZDvU"
   },
   "source": [
    "# Default title text\n",
    "# RAG System - Asistente de InversiÃ³n en Dividendos\n",
    "\n",
    "Este notebook implementa un sistema completo de **Retrieval-Augmented Generation (RAG)** especializado en inversiÃ³n en dividendos.\n",
    "\n",
    "## ğŸ¯ Objetivo\n",
    "Desarrollar un asistente inteligente capaz de responder preguntas especÃ­ficas sobre estrategias de inversiÃ³n en dividendos, anÃ¡lisis financiero y gestiÃ³n de carteras, basÃ¡ndose en un extenso corpus de conocimiento especializado.\n",
    "\n",
    "## ğŸ“š Dataset de Conocimiento Financiero\n",
    "- **93 documentos** de contenido financiero especializado\n",
    "- **Pipeline de procesamiento avanzado**: Audio â†’ TranscripciÃ³n â†’ OptimizaciÃ³n con IA â†’ Textos estructurados\n",
    "- **Ãreas de conocimiento**: AnÃ¡lisis fundamental, selecciÃ³n de brokers, psicologÃ­a de la inversiÃ³n, gestiÃ³n de riesgo, fiscalidad, ratios financieros, etc.\n",
    "- **Estructura modular**: Contenido organizado en 16 mÃ³dulos temÃ¡ticos + material complementario\n",
    "\n",
    "## ğŸ”§ Stack TecnolÃ³gico\n",
    "- **LangChain**: Framework para aplicaciones RAG\n",
    "- **OpenAI**: Embeddings (text-embedding-3-large) y LLM (GPT-4o-mini)\n",
    "- **FAISS**: Base de datos vectorial para bÃºsqueda semÃ¡ntica eficiente\n",
    "- **Python**: Procesamiento de datos y anÃ¡lisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kze0ZANvZMXU"
   },
   "source": [
    "## ğŸ“¦ InstalaciÃ³n de Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCvCmBoxYzYL"
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain-openai langchain-community faiss-cpu tiktoken tqdm pandas matplotlib seaborn openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqObnBg0gwsH"
   },
   "source": [
    "## ğŸ’¾ ConfiguraciÃ³n de Google Drive y VerificaciÃ³n de Chunks\n",
    "\n",
    "Montamos Google Drive para persistir los chunks procesados y evitar repetir el proceso de extracciÃ³n de metadatos con GPT-4o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zz5IwrdKgwsH"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "print(\"ğŸ“ Montando Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "print(\"âœ… Google Drive montado correctamente\")\n",
    "\n",
    "DRIVE_FOLDER = '/content/drive/MyDrive/RAG_Dividendos/'\n",
    "CHUNKS_PATH = '/content/drive/MyDrive/RAG_Dividendos/chunks_procesados.pkl'\n",
    "\n",
    "os.makedirs(DRIVE_FOLDER, exist_ok=True)\n",
    "print(f\"ğŸ“‚ Carpeta de trabajo: {DRIVE_FOLDER}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ” Verificar si ya existen chunks procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJJliBcTgwsI"
   },
   "outputs": [],
   "source": [
    "print(\"ğŸ” Verificando chunks existentes en Google Drive...\")\n",
    "\n",
    "chunks_exist = os.path.exists(CHUNKS_PATH)\n",
    "\n",
    "if chunks_exist:\n",
    "    print(\"âœ… Â¡Chunks encontrados en Google Drive!\")\n",
    "    print(f\"ğŸ“ Ruta: {CHUNKS_PATH}\")\n",
    "\n",
    "    try:\n",
    "        print(\"ğŸ“¥ Cargando chunks desde Google Drive...\")\n",
    "        with open(CHUNKS_PATH, 'rb') as f:\n",
    "            chunks = pickle.load(f)\n",
    "\n",
    "        print(f\"ğŸ‰ Â¡Chunks cargados exitosamente!\")\n",
    "        print(f\"   ğŸ“Š Total de chunks: {len(chunks)}\")\n",
    "        print(f\"   ğŸ“š Documentos Ãºnicos: {len(set([chunk.metadata['source_file'] for chunk in chunks]))}\")\n",
    "\n",
    "        if chunks:\n",
    "            sample_chunk = chunks[0]\n",
    "            has_metadata = 'main_topic' in sample_chunk.metadata and 'level' in sample_chunk.metadata\n",
    "\n",
    "            if has_metadata:\n",
    "                print(f\"âœ… Metadatos verificados - chunks listos para usar\")\n",
    "                print(f\"\\nğŸ“‹ Ejemplo de chunk cargado:\")\n",
    "                print(f\"   ğŸ†” Chunk ID: {sample_chunk.metadata.get('chunk_id', 'N/A')}\")\n",
    "                print(f\"   ğŸ“ Archivo: {sample_chunk.metadata.get('filename', 'N/A')}\")\n",
    "                print(f\"   ğŸ¯ Tema: {sample_chunk.metadata.get('main_topic', 'N/A')}\")\n",
    "                print(f\"   ğŸ“Š Nivel: {sample_chunk.metadata.get('level', 'N/A')}\")\n",
    "                print(f\"   ğŸ”¤ Tokens: {sample_chunk.metadata.get('chunk_tokens', 'N/A')}\")\n",
    "\n",
    "                print(f\"\\nâš¡ SALTANDO TODO EL PROCESAMIENTO - chunks listos para RAG\")\n",
    "                SKIP_PROCESSING = True\n",
    "            else:\n",
    "                print(f\"âš ï¸ Chunks encontrados pero sin metadatos completos\")\n",
    "                print(f\"ğŸ”„ Se ejecutarÃ¡ el procesamiento completo\")\n",
    "                SKIP_PROCESSING = False\n",
    "        else:\n",
    "            print(f\"âŒ Archivo de chunks vacÃ­o\")\n",
    "            SKIP_PROCESSING = False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error cargando chunks: {e}\")\n",
    "        print(f\"ğŸ”„ Se ejecutarÃ¡ el procesamiento completo\")\n",
    "        SKIP_PROCESSING = False\n",
    "        chunks = None\n",
    "\n",
    "else:\n",
    "    print(\"âŒ No se encontraron chunks en Google Drive\")\n",
    "    print(f\"ğŸ”„ Se ejecutarÃ¡ el procesamiento completo desde cero\")\n",
    "    SKIP_PROCESSING = False\n",
    "    chunks = None\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "if SKIP_PROCESSING:\n",
    "    print(\"ğŸš€ MODO RÃPIDO: Usando chunks existentes\")\n",
    "    print(\"ğŸ’¡ Puedes saltar directamente a la implementaciÃ³n del RAG\")\n",
    "else:\n",
    "    print(\"ğŸ”„ MODO PROCESAMIENTO: Generando chunks desde cero\")\n",
    "    print(\"ğŸ’¡ Ejecuta las siguientes celdas para procesar el dataset\")\n",
    "print(f\"{'='*50}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljaBkIW4ZWCD"
   },
   "source": [
    "## ğŸ”‘ ConfiguraciÃ³n de API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dozJZCoCYzYM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Introduce tu OpenAI API Key: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7IkrzfpZaaz"
   },
   "source": [
    "## ğŸ“Š AnÃ¡lisis del Dataset de Conocimiento Financiero\n",
    "\n",
    "En esta secciÃ³n analizaremos la estructura y caracterÃ­sticas del dataset procesado, que contiene conocimiento especializado sobre inversiÃ³n en dividendos.\n",
    "\n",
    "### ğŸµ Pipeline de Procesamiento del Dataset\n",
    "\n",
    "Antes de proceder con el anÃ¡lisis, es importante entender cÃ³mo se ha generado este dataset de conocimiento financiero:\n",
    "\n",
    "#### **1. ğŸ™ï¸ ExtracciÃ³n de Audio a Texto**\n",
    "- **Fuente original**: Contenido de audio especializado en inversiÃ³n en dividendos\n",
    "- **Herramienta utilizada**: OpenAI Whisper para transcripciÃ³n automÃ¡tica\n",
    "- **Resultado**: Transcripciones en bruto de los audios\n",
    "\n",
    "#### **2. âœ‚ï¸ SegmentaciÃ³n Inicial**\n",
    "- **Objetivo**: Dividir el contenido en fragmentos manejables\n",
    "- **LÃ­mite**: Respetar los lÃ­mites de tokens de la API de OpenAI\n",
    "- **MÃ©todo**: Chunking preservando contexto (4.000 tokens - 300 overlap tokens) - (GPT-4o ~4.096 tokens mÃ¡ximos en la respuesta)\n",
    "\n",
    "#### **3. ğŸ§  Limpieza y OptimizaciÃ³n con IA**\n",
    "- **Herramienta**: GPT-4o\n",
    "- **Proceso**: Refinamiento del contenido transcrito para mejorar claridad y estructura\n",
    "- **Prompt utilizado**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "source": [
    "Eres un redactor profesional especializado en convertir transcripciones de voz en textos escritos claros, concisos y de alta calidad para publicaciones financieras.\n",
    "\n",
    "Tienes como entrada la transcripciÃ³n de una presentaciÃ³n sobre inversiÃ³n en dividendos. Tu tarea es reescribir el texto para que:\n",
    "\n",
    "ELIMINAR COMPLETAMENTE:\n",
    "- Muletillas, repeticiones, frases interrumpidas y expresiones orales (como \"ehh\", \"vale\", \"como decÃ­amos\", \"bueno\", \"entonces\").\n",
    "- Referencias a cursos, mÃ³dulos, lecciones, clases, alumnos, estudiantes o cualquier contexto educativo.\n",
    "- Instrucciones tÃ©cnicas de plataformas (como \"haz clic\", \"descarga\", \"deja tu comentario\").\n",
    "- Expresiones dirigidas directamente al lector (\"tÃº\", \"vosotros\", \"ustedes\").\n",
    "- Markdown innecesario, emojis decorativos, saltos de lÃ­nea excesivos (`<br>`).\n",
    "\n",
    "TRANSFORMAR EN:\n",
    "- Un texto profesional de divulgaciÃ³n financiera, como si fuera parte de un libro especializado o artÃ­culo de revista econÃ³mica.\n",
    "- RedacciÃ³n en tercera persona o impersonal, con tono objetivo y profesional.\n",
    "- Estructura clara con pÃ¡rrafos bien organizados.\n",
    "- TerminologÃ­a tÃ©cnica precisa sin simplificaciones excesivas.\n",
    "\n",
    "MANTENER:\n",
    "- Toda la informaciÃ³n tÃ©cnica y financiera relevante sobre dividendos.\n",
    "- Datos, cifras, ejemplos prÃ¡cticos y estrategias de inversiÃ³n.\n",
    "- Conceptos, definiciones y explicaciones tÃ©cnicas.\n",
    "\n",
    "MANEJO DE SOLAPAMIENTO:\n",
    "Entre las etiquetas `{overlap_start}` y `{overlap_end}` encontrarÃ¡s texto que ya apareciÃ³ en el bloque anterior:\n",
    "- No lo reescribas a menos que sea necesario para la continuidad.\n",
    "- Evita repetir ideas ya tratadas.\n",
    "\n",
    "Si encuentras alguna palabra, frase o fragmento que no entiendas o no puedas mejorar o contenido que no puedas procesar adecuadamente, dÃ©jalo tal cual pero mÃ¡rcalo entre etiquetas `<unprocessed>...texto...</unprocessed>` para que pueda revisarse manualmente mÃ¡s adelante.\n",
    "\n",
    "No inventes informaciÃ³n nueva. Reescribe el texto original, pero mejora la redacciÃ³n y elimina el contenido innecesario.\n",
    "El resultado debe ser un texto que parezca extraÃ­do de una publicaciÃ³n financiera profesional, sin rastro de su origen como material educativo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **4. ğŸ“ EstructuraciÃ³n Final**\n",
    "- **OrganizaciÃ³n**: Un archivo .txt por lecciÃ³n\n",
    "- **Formato**: Texto plano listo para chunking con LangChain\n",
    "\n",
    "### ğŸ”„ Pipeline RAG en este Notebook\n",
    "\n",
    "En este notebook completaremos el pipeline RAG:\n",
    "\n",
    "1. **ğŸ“š Document Loading**: Cargar archivos .txt con LangChain\n",
    "2. **âœ‚ï¸ Chunking**: SegmentaciÃ³n con RecursiveCharacterTextSplitter\n",
    "3. **ğŸ§  Metadata Extraction**: Usar GPT-4o para extraer `level` y `main_topic` por documento\n",
    "4. **ğŸ—„ï¸ Vector Store**: Crear Ã­ndice semÃ¡ntico con FAISS y embeddings OpenAI\n",
    "5. **ğŸ” Retrieval**: Sistema de bÃºsqueda semÃ¡ntica con filtros por metadatos\n",
    "6. **ğŸ¤– RAG Pipeline**: Cadena completa de pregunta-respuesta con GPT-4o-mini\n",
    "\n",
    "### ğŸ“‚ Carga del Dataset Procesado\n",
    "\n",
    "Para ejecutar este notebook, necesitas subir los archivos de texto procesados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwAujE1qaCjA"
   },
   "source": [
    "### ğŸ“‹ Instrucciones para subir el dataset\n",
    "\n",
    "**ğŸ“¦ Pasos para cargar los archivos de texto procesados:**\n",
    "1. ğŸ“ Comprime los archivos .txt procesados en un archivo llamado `data.zip`\n",
    "2. ğŸ“¤ Haz clic en el icono de carpeta ğŸ“ en la barra lateral izquierda\n",
    "3. ğŸ”„ Arrastra y suelta `data.zip` o usa \"Subir archivos\"\n",
    "4. â³ Espera a que se complete la subida\n",
    "5. â–¶ï¸ Ejecuta las celdas siguientes para:\n",
    "   - Extraer y cargar documentos con LangChain\n",
    "   - Aplicar chunking\n",
    "   - Extraer metadatos con GPT-4o (`level` y `main_topic`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8QP2I5CYzYM"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“¤ Extraer y cargar archivos del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYbLW_YNYzYN"
   },
   "outputs": [],
   "source": [
    "if not SKIP_PROCESSING:\n",
    "    print(\"ğŸ” Buscando archivos del dataset...\")\n",
    "\n",
    "    if os.path.exists('data.zip'):\n",
    "        print(\"âœ… Archivo data.zip encontrado. Extrayendo...\")\n",
    "        with zipfile.ZipFile('data.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall('.')\n",
    "        print(\"âœ… Archivos extraÃ­dos correctamente\")\n",
    "    else:\n",
    "        print(\"âŒ No se encontrÃ³ data.zip\")\n",
    "        print(\"\\nğŸ“‹ PASOS A SEGUIR:\")\n",
    "        print(\"1. Ve al panel de archivos (ğŸ“) en la barra lateral izquierda\")\n",
    "        print(\"2. Haz clic en 'Subir archivos' o arrastra data.zip\")\n",
    "        print(\"3. Espera a que se complete la subida\")\n",
    "        print(\"4. Vuelve a ejecutar esta celda\")\n",
    "\n",
    "    txt_files = glob.glob(\"data/*.txt\")\n",
    "    print(f\"\\nğŸ“ Archivos de texto encontrados en 'data/': {len(txt_files)}\")\n",
    "\n",
    "    if len(txt_files) == 0:\n",
    "        print(\"âŒ No se encontraron archivos de texto en la carpeta 'data'\")\n",
    "        print(\"ğŸ”„ AsegÃºrate de haber subido data.zip correctamente y que contenga la carpeta 'data' con archivos .txt\")\n",
    "    else:\n",
    "        print(f\"âœ… Dataset encontrado: {len(txt_files)} lecciones de conocimiento financiero\")\n",
    "else:\n",
    "    print(\"âš¡ SALTANDO: Ya tenemos chunks procesados desde Google Drive\")\n",
    "    txt_files = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6XAlEqMaP6u"
   },
   "source": [
    "## ğŸ“š Document Loading con LangChain\n",
    "\n",
    "En esta secciÃ³n cargaremos los documentos de texto utilizando LangChain y extraeremos metadatos desde los nombres de archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elnCxY0lYzYN"
   },
   "outputs": [],
   "source": [
    "if not SKIP_PROCESSING:\n",
    "    import re\n",
    "    from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "    from langchain.schema import Document\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    import tiktoken\n",
    "\n",
    "    def extract_metadata_from_filename(filename):\n",
    "        base_name = filename.replace('.txt', '')\n",
    "        parts = base_name.split('_')\n",
    "\n",
    "        if len(parts) >= 2:\n",
    "            try:\n",
    "                module = int(parts[0])\n",
    "                lesson = int(parts[1])\n",
    "\n",
    "                if 'bonus' in base_name.lower():\n",
    "                    content_type = \"bonus\"\n",
    "                elif 'conclusion' in base_name.lower():\n",
    "                    content_type = \"conclusiÃ³n\"\n",
    "                elif lesson == 0:\n",
    "                    content_type = \"introducciÃ³n\"\n",
    "                else:\n",
    "                    content_type = \"lecciÃ³n\"\n",
    "\n",
    "                return {\n",
    "                    'source_file': base_name,\n",
    "                    'module': module,\n",
    "                    'lesson': lesson,\n",
    "                    'content_type': content_type,\n",
    "                    'filename': filename\n",
    "                }\n",
    "            except (ValueError, IndexError):\n",
    "                pass\n",
    "\n",
    "        return {\n",
    "            'source_file': base_name,\n",
    "            'module': 0,\n",
    "            'lesson': 0,\n",
    "            'content_type': 'contenido',\n",
    "            'filename': filename\n",
    "        }\n",
    "\n",
    "    print(\"ğŸ”§ Funciones de extracciÃ³n de metadatos configuradas\")\n",
    "else:\n",
    "    print(\"âš¡ SALTANDO: Funciones de procesamiento no necesarias\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“– Cargar documentos con LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rubPWSMYzYN"
   },
   "outputs": [],
   "source": [
    "if not SKIP_PROCESSING:\n",
    "    print(\"ğŸ“š Cargando documentos con LangChain...\")\n",
    "\n",
    "    if len(txt_files) > 0:\n",
    "        raw_documents = []\n",
    "        for txt_file in txt_files:\n",
    "            try:\n",
    "                loader = TextLoader(txt_file, encoding='utf-8')\n",
    "                doc = loader.load()[0]\n",
    "                raw_documents.append(doc)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error cargando {txt_file}: {e}\")\n",
    "\n",
    "        print(f\"âœ… Cargados {len(raw_documents)} documentos base\")\n",
    "\n",
    "        enriched_documents = []\n",
    "        for doc in raw_documents:\n",
    "            filename = os.path.basename(doc.metadata['source'])\n",
    "            basic_metadata = extract_metadata_from_filename(filename)\n",
    "            doc.metadata.update(basic_metadata)\n",
    "            enriched_documents.append(doc)\n",
    "\n",
    "        print(f\"âœ… Documentos enriquecidos con metadatos bÃ¡sicos\")\n",
    "\n",
    "        if enriched_documents:\n",
    "            sample_doc = enriched_documents[0]\n",
    "            print(f\"\\nğŸ“‹ Ejemplo de metadatos bÃ¡sicos:\")\n",
    "            print(f\"ğŸ“ Archivo: {sample_doc.metadata['filename']}\")\n",
    "            print(f\"ğŸ“š MÃ³dulo: {sample_doc.metadata['module']}\")\n",
    "            print(f\"ğŸ“– LecciÃ³n: {sample_doc.metadata['lesson']}\")\n",
    "            print(f\"ğŸ”– Tipo: {sample_doc.metadata['content_type']}\")\n",
    "            content_preview = sample_doc.page_content[:200] + \"...\" if len(sample_doc.page_content) > 200 else sample_doc.page_content\n",
    "            print(f\"\\nğŸ“ Vista previa del contenido:\")\n",
    "            print(f\"'{content_preview}'\")\n",
    "\n",
    "            print(f\"\\nğŸ’¡ Nota: Los metadatos 'level' y 'main_topic' se extraerÃ¡n con GPT-4o automÃ¡ticamente\")\n",
    "    else:\n",
    "        print(\"âŒ No hay documentos para cargar\")\n",
    "else:\n",
    "    print(\"âš¡ SALTANDO: Carga de documentos no necesaria\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSITPxIeaYzf"
   },
   "source": [
    "## âœ‚ï¸ Chunking Strategy con LangChain\n",
    "\n",
    "Implementaremos una estrategia de chunking utilizando el `RecursiveCharacterTextSplitter` de LangChain, optimizado para contenido financiero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ‚ï¸ Configurar Text Splitter optimizado para contenido financiero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8lUD1UPYzYN"
   },
   "outputs": [],
   "source": [
    "if not SKIP_PROCESSING:\n",
    "    print(\"ğŸ”§ Configurando estrategia de chunking...\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=150,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \"],\n",
    "        add_start_index=True\n",
    "    )\n",
    "\n",
    "    print(\"âœ… Text Splitter configurado:\")\n",
    "    print(f\"   ğŸ“ Chunk size: {text_splitter._chunk_size}\")\n",
    "    print(f\"   ğŸ”— Overlap: {text_splitter._chunk_overlap}\")\n",
    "    print(f\"   ğŸ“Š Separadores: {len(text_splitter._separators)} niveles\")\n",
    "else:\n",
    "    print(\"âš¡ SALTANDO: ConfiguraciÃ³n de chunking no necesaria\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ”„ Aplicar chunking a los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQ69ZFLaYzYN"
   },
   "outputs": [],
   "source": [
    "if not SKIP_PROCESSING:\n",
    "    print(\"âœ‚ï¸ Aplicando chunking a los documentos...\")\n",
    "\n",
    "    if 'enriched_documents' in locals() and enriched_documents:\n",
    "        chunks = text_splitter.split_documents(enriched_documents)\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk.metadata['chunk_id'] = i\n",
    "            chunk.metadata['chunk_length'] = len(chunk.page_content)\n",
    "\n",
    "            try:\n",
    "                encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "                chunk.metadata['chunk_tokens'] = len(encoding.encode(chunk.page_content))\n",
    "            except:\n",
    "                chunk.metadata['chunk_tokens'] = len(chunk.page_content) // 4\n",
    "\n",
    "        print(f\"âœ… Chunking completado:\")\n",
    "        print(f\"   ğŸ“š Documentos originales: {len(enriched_documents)}\")\n",
    "        print(f\"   ğŸ§© Chunks generados: {len(chunks)}\")\n",
    "        print(f\"   ğŸ“Š Promedio chunks por documento: {len(chunks) / len(enriched_documents):.1f}\")\n",
    "\n",
    "        chunk_lengths = [chunk.metadata['chunk_length'] for chunk in chunks]\n",
    "        chunk_tokens = [chunk.metadata['chunk_tokens'] for chunk in chunks]\n",
    "\n",
    "        print(f\"\\nğŸ“ˆ EstadÃ­sticas de chunks:\")\n",
    "        print(f\"   ğŸ“ Longitud promedio: {sum(chunk_lengths) / len(chunk_lengths):.0f} caracteres\")\n",
    "        print(f\"   ğŸ”¤ Tokens promedio: {sum(chunk_tokens) / len(chunk_tokens):.0f} tokens\")\n",
    "        print(f\"   ğŸ“Š Longitud mÃ­n/mÃ¡x: {min(chunk_lengths)}/{max(chunk_lengths)} caracteres\")\n",
    "        print(f\"   ğŸ¯ Tokens mÃ­n/mÃ¡x: {min(chunk_tokens)}/{max(chunk_tokens)} tokens\")\n",
    "\n",
    "        if chunks:\n",
    "            sample_chunk = chunks[0]\n",
    "            print(sample_chunk)\n",
    "            print(f\"\\nğŸ“‹ Ejemplo de chunk generado:\")\n",
    "            print(f\"   ğŸ†” Chunk ID: {sample_chunk.metadata['chunk_id']}\")\n",
    "            print(f\"   ğŸ“ Documento origen: {sample_chunk.metadata['filename']}\")\n",
    "            print(f\"   ğŸ“ Longitud: {sample_chunk.metadata['chunk_length']} caracteres\")\n",
    "            print(f\"   ğŸ”¤ Tokens: {sample_chunk.metadata['chunk_tokens']} tokens\")\n",
    "            preview = sample_chunk.page_content[:300] + \"...\" if len(sample_chunk.page_content) > 300 else sample_chunk.page_content\n",
    "            print(f\"   ğŸ“ Contenido: '{preview}'\")\n",
    "\n",
    "    else:\n",
    "        print(\"âŒ No hay documentos cargados para hacer chunking\")\n",
    "else:\n",
    "    print(\"âš¡ SALTANDO: Chunking no necesario\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤– ExtracciÃ³n de Metadatos con GPT-4o\n",
    "\n",
    "Esta secciÃ³n utiliza **GPT-4o** para analizar automÃ¡ticamente el contenido de cada documento y extraer metadatos clave:\n",
    "\n",
    "- **ğŸ“‹ main_topic**: Identifica el tema principal de cada lecciÃ³n (ej: \"Ventajas de invertir en dividendos\")\n",
    "- **ğŸ“Š level**: Determina el nivel de dificultad del contenido (bÃ¡sico, intermedio, avanzado)\n",
    "\n",
    "Estos metadatos enriquecen los chunks y permiten filtros mÃ¡s precisos durante la recuperaciÃ³n de informaciÃ³n en el sistema RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QvNgZ2D6YzYO"
   },
   "outputs": [],
   "source": [
    "if not SKIP_PROCESSING:\n",
    "    from openai import OpenAI\n",
    "    import tiktoken\n",
    "    import json as json_lib\n",
    "    from collections import defaultdict\n",
    "\n",
    "    client = OpenAI()\n",
    "\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "    def count_tokens(text):\n",
    "        return len(encoding.encode(text))\n",
    "\n",
    "    def truncate_text_if_needed(text, max_tokens=8000):\n",
    "        tokens = encoding.encode(text)\n",
    "        if len(tokens) <= max_tokens:\n",
    "            return text, len(tokens)\n",
    "\n",
    "        truncated_tokens = tokens[:max_tokens]\n",
    "        truncated_text = encoding.decode(truncated_tokens)\n",
    "        print(f\"âš ï¸  Texto truncado de {len(tokens)} a {len(truncated_tokens)} tokens\")\n",
    "        return truncated_text, len(truncated_tokens)\n",
    "\n",
    "    def analyze_content_with_gpt4o(text):\n",
    "        final_text, num_tokens = truncate_text_if_needed(text)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Eres un asistente experto en educaciÃ³n financiera. Se te da el contenido de una lecciÃ³n sobre inversiÃ³n en dividendos.\n",
    "\n",
    "        Tu tarea es:\n",
    "        1. Identificar el tema principal (una frase corta, como \"Ventajas de invertir en dividendos\").\n",
    "        2. Estimar el nivel de dificultad del contenido (bÃ¡sico, intermedio o avanzado).\n",
    "\n",
    "        Contenido de la lecciÃ³n ({num_tokens} tokens):\n",
    "        \\\"\\\"\\\"\\n{final_text}\\n\\\"\\\"\\\"\n",
    "\n",
    "        RESPONDE ÃšNICAMENTE CON EL JSON, SIN MARKDOWN, SIN EXPLICACIONES, SIN TEXTO ADICIONAL:\n",
    "\n",
    "        {{\n",
    "            \"main_topic\": \"...\",\n",
    "            \"level\": \"bÃ¡sico | intermedio | avanzado\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.1,\n",
    "                    max_tokens=150\n",
    "                )\n",
    "\n",
    "                response_text = response.choices[0].message.content.strip()\n",
    "                \n",
    "                try:\n",
    "                    return json_lib.loads(response_text)\n",
    "                except json_lib.JSONDecodeError:\n",
    "                    import re\n",
    "                    json_match = re.search(r'\\{[^}]+\\}', response_text)\n",
    "                    if json_match:\n",
    "                        try:\n",
    "                            return json_lib.loads(json_match.group())\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    if attempt < max_retries - 1:\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "        return {\"main_topic\": \"Contenido financiero\", \"level\": \"intermedio\"}\n",
    "\n",
    "    print(\"ğŸ”§ Funciones de extracciÃ³n de metadatos con GPT-4o configuradas\")\n",
    "else:\n",
    "    print(\"âš¡ SALTANDO: Funciones de GPT-4o no necesarias\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ§  Extraer metadatos por documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClZFt6z7YzYO"
   },
   "outputs": [],
   "source": [
    "if not SKIP_PROCESSING:\n",
    "    print(\"ğŸ¤– Extrayendo metadatos con GPT-4o...\")\n",
    "\n",
    "    if 'chunks' in locals() and chunks:\n",
    "        docs_by_source = defaultdict(list)\n",
    "        for chunk in chunks:\n",
    "            source_file = chunk.metadata['source_file']\n",
    "            docs_by_source[source_file].append(chunk)\n",
    "\n",
    "        print(f\"ğŸ“š Procesando {len(docs_by_source)} documentos Ãºnicos...\")\n",
    "\n",
    "        metadata_cache = {}\n",
    "\n",
    "        for i, (source_file, doc_chunks) in enumerate(docs_by_source.items()):\n",
    "            print(f\"\\nğŸ”„ Procesando ({i+1}/{len(docs_by_source)}): {source_file}\")\n",
    "            full_content = \" \".join([chunk.page_content for chunk in doc_chunks])\n",
    "\n",
    "            try:\n",
    "                metadata = analyze_content_with_gpt4o(full_content)\n",
    "                metadata_cache[source_file] = metadata\n",
    "\n",
    "                print(f\"   âœ… Tema: {metadata['main_topic']}\")\n",
    "                print(f\"   ğŸ“Š Nivel: {metadata['level']}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Error: {e}\")\n",
    "                metadata_cache[source_file] = {\n",
    "                    \"main_topic\": \"Contenido financiero\",\n",
    "                    \"level\": \"intermedio\"\n",
    "                }\n",
    "\n",
    "        print(f\"\\nğŸ”„ Aplicando metadatos a {len(chunks)} chunks...\")\n",
    "\n",
    "        for chunk in chunks:\n",
    "            source_file = chunk.metadata['source_file']\n",
    "            if source_file in metadata_cache:\n",
    "                chunk.metadata['main_topic'] = metadata_cache[source_file]['main_topic']\n",
    "                chunk.metadata['level'] = metadata_cache[source_file]['level']\n",
    "\n",
    "        print(f\"âœ… Metadatos aplicados a todos los chunks\")\n",
    "\n",
    "        if chunks:\n",
    "            sample_chunk = chunks[0]\n",
    "            print(f\"\\nğŸ“‹ Ejemplo de chunk con metadatos completos:\")\n",
    "            print(f\"   ğŸ†” Chunk ID: {sample_chunk.metadata['chunk_id']}\")\n",
    "            print(f\"   ğŸ“ Archivo: {sample_chunk.metadata['filename']}\")\n",
    "            print(f\"   ğŸ“š MÃ³dulo: {sample_chunk.metadata['module']}\")\n",
    "            print(f\"   ğŸ“– LecciÃ³n: {sample_chunk.metadata['lesson']}\")\n",
    "            print(f\"   ğŸ¯ Tema: {sample_chunk.metadata['main_topic']}\")\n",
    "            print(f\"   ğŸ“Š Nivel: {sample_chunk.metadata['level']}\")\n",
    "            print(f\"   ğŸ”– Tipo: {sample_chunk.metadata['content_type']}\")\n",
    "            print(f\"   ğŸ”¤ Tokens: {sample_chunk.metadata['chunk_tokens']}\")\n",
    "\n",
    "        print(f\"\\nğŸ’¾ Guardando chunks procesados en Google Drive...\")\n",
    "        try:\n",
    "            with open(CHUNKS_PATH, 'wb') as f:\n",
    "                pickle.dump(chunks, f)\n",
    "            print(f\"âœ… {len(chunks)} chunks guardados exitosamente en Google Drive\")\n",
    "            print(f\"ğŸ“ Ruta: {CHUNKS_PATH}\")\n",
    "            print(f\"ğŸ’¡ En futuras ejecuciones se cargarÃ¡n automÃ¡ticamente desde aquÃ­\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error guardando chunks: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(\"âŒ No hay chunks disponibles para extraer metadatos\")\n",
    "else:\n",
    "    print(\"âš¡ SALTANDO: ExtracciÃ³n de metadatos no necesaria\")\n",
    "\n",
    "print(\"âœ… Â¡Dataset listo para implementar el sistema RAG completo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ—„ï¸ Vector Store con FAISS\n",
    "\n",
    "Implementaremos el vector store usando FAISS para bÃºsqueda semÃ¡ntica sobre nuestros chunks procesados con metadatos completos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ’¾ VerificaciÃ³n de Vector Store existente en Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” Verificando vector store existente en Google Drive...\")\n",
    "\n",
    "VECTORSTORE_PATH = '/content/drive/MyDrive/RAG_Dividendos/vectorstore_faiss'\n",
    "VECTORSTORE_INDEX_PATH = '/content/drive/MyDrive/RAG_Dividendos/vectorstore_faiss/index.faiss'\n",
    "VECTORSTORE_PKL_PATH = '/content/drive/MyDrive/RAG_Dividendos/vectorstore_faiss/index.pkl'\n",
    "\n",
    "vectorstore_exists = os.path.exists(VECTORSTORE_INDEX_PATH) and os.path.exists(VECTORSTORE_PKL_PATH)\n",
    "\n",
    "if vectorstore_exists:\n",
    "    print(\"âœ… Â¡Vector store encontrado en Google Drive!\")\n",
    "    print(f\"ğŸ“ Ãndice FAISS: {VECTORSTORE_INDEX_PATH}\")\n",
    "    print(f\"ğŸ“ Metadatos: {VECTORSTORE_PKL_PATH}\")\n",
    "    \n",
    "    try:\n",
    "        from langchain_community.vectorstores import FAISS\n",
    "        from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "        \n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "        \n",
    "        print(\"ğŸ“¥ Cargando vector store desde Google Drive...\")\n",
    "        vectorstore = FAISS.load_local(\n",
    "            VECTORSTORE_PATH, \n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        \n",
    "        print(f\"ğŸ‰ Â¡Vector store cargado exitosamente!\")\n",
    "        print(f\"   ğŸ“Š Vectores en el Ã­ndice: {vectorstore.index.ntotal}\")\n",
    "        print(f\"   ğŸ“ Dimensiones: {vectorstore.index.d}\")\n",
    "        \n",
    "        print(f\"\\nâš¡ SALTANDO CREACIÃ“N DE VECTOR STORE - usando existente\")\n",
    "        SKIP_VECTORSTORE = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error cargando vector store: {e}\")\n",
    "        print(f\"ğŸ”„ Se crearÃ¡ un nuevo vector store\")\n",
    "        SKIP_VECTORSTORE = False\n",
    "        vectorstore = None\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ No se encontrÃ³ vector store en Google Drive\")\n",
    "    print(f\"ğŸ”„ Se crearÃ¡ un nuevo vector store desde cero\")\n",
    "    SKIP_VECTORSTORE = False\n",
    "    vectorstore = None\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "if SKIP_VECTORSTORE:\n",
    "    print(\"ğŸš€ MODO RÃPIDO: Usando vector store existente\")\n",
    "    print(\"ğŸ’¡ Listo para bÃºsquedas semÃ¡nticas\")\n",
    "else:\n",
    "    print(\"ğŸ”„ MODO CREACIÃ“N: Generando vector store desde cero\")\n",
    "    print(\"ğŸ’¡ Se crearÃ¡n embeddings para todos los chunks\")\n",
    "print(f\"{'='*50}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ§  CreaciÃ³n del Vector Store con FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_VECTORSTORE:\n",
    "    print(\"ğŸ”§ Creando vector store con FAISS...\")\n",
    "    \n",
    "    if 'chunks' in locals() and chunks and len(chunks) > 0:\n",
    "        print(f\"ğŸ“Š Procesando {len(chunks)} chunks para crear embeddings...\")\n",
    "        \n",
    "        from langchain_community.vectorstores import FAISS\n",
    "        from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "        import time\n",
    "        \n",
    "        print(\"ğŸ”§ Inicializando OpenAI Embeddings (text-embedding-3-large)...\")\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "        \n",
    "        print(\"â³ Creando embeddings y construyendo Ã­ndice FAISS...\")\n",
    "        print(\"ğŸ’¡ Este proceso puede tomar varios minutos dependiendo del nÃºmero de chunks...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            vectorstore = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            processing_time = end_time - start_time\n",
    "            \n",
    "            print(f\"âœ… Vector store creado exitosamente!\")\n",
    "            print(f\"   â±ï¸ Tiempo de procesamiento: {processing_time:.1f} segundos\")\n",
    "            print(f\"   ğŸ“Š Vectores en el Ã­ndice: {vectorstore.index.ntotal}\")\n",
    "            print(f\"   ğŸ“ Dimensiones del embedding: {vectorstore.index.d}\")\n",
    "            print(f\"   ğŸ§© Chunks procesados: {len(chunks)}\")\n",
    "            \n",
    "            if chunks:\n",
    "                sample_doc = vectorstore.similarity_search(\"dividendos\", k=1)[0]\n",
    "                print(f\"\\nğŸ“‹ VerificaciÃ³n de metadatos en vector store:\")\n",
    "                print(f\"   ğŸ“ Archivo: {sample_doc.metadata.get('filename', 'N/A')}\")\n",
    "                print(f\"   ğŸ¯ Tema: {sample_doc.metadata.get('main_topic', 'N/A')}\")\n",
    "                print(f\"   ğŸ“Š Nivel: {sample_doc.metadata.get('level', 'N/A')}\")\n",
    "                print(f\"   ğŸ“š MÃ³dulo: {sample_doc.metadata.get('module', 'N/A')}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error creando vector store: {e}\")\n",
    "            print(\"ğŸ”„ Verifica tu API key de OpenAI y conexiÃ³n a internet\")\n",
    "            vectorstore = None\n",
    "            \n",
    "    else:\n",
    "        print(\"âŒ No hay chunks disponibles para crear el vector store\")\n",
    "        print(\"ğŸ”„ AsegÃºrate de haber ejecutado las celdas de procesamiento de chunks\")\n",
    "        vectorstore = None\n",
    "        \n",
    "else:\n",
    "    print(\"âš¡ SALTANDO: Vector store ya cargado desde Google Drive\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ’¾ Persistencia del Vector Store en Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_VECTORSTORE and vectorstore is not None:\n",
    "    print(\"ğŸ’¾ Guardando vector store en Google Drive...\")\n",
    "    \n",
    "    try:\n",
    "        vectorstore.save_local(VECTORSTORE_PATH)\n",
    "        \n",
    "        print(f\"âœ… Vector store guardado exitosamente en Google Drive!\")\n",
    "        print(f\"ğŸ“ Ruta base: {VECTORSTORE_PATH}\")\n",
    "        print(f\"ğŸ“ Ãndice FAISS: {VECTORSTORE_INDEX_PATH}\")\n",
    "        print(f\"ğŸ“ Metadatos: {VECTORSTORE_PKL_PATH}\")\n",
    "        print(f\"ğŸ’¡ En futuras ejecuciones se cargarÃ¡ automÃ¡ticamente desde aquÃ­\")\n",
    "        \n",
    "        if os.path.exists(VECTORSTORE_INDEX_PATH) and os.path.exists(VECTORSTORE_PKL_PATH):\n",
    "            index_size = os.path.getsize(VECTORSTORE_INDEX_PATH) / (1024*1024)\n",
    "            pkl_size = os.path.getsize(VECTORSTORE_PKL_PATH) / (1024*1024)\n",
    "            print(f\"ğŸ“Š TamaÃ±o del Ã­ndice: {index_size:.1f} MB\")\n",
    "            print(f\"ğŸ“Š TamaÃ±o de metadatos: {pkl_size:.1f} MB\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Advertencia: No se pudieron verificar todos los archivos guardados\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error guardando vector store: {e}\")\n",
    "        print(\"âš ï¸ El vector store se ha creado pero no se pudo guardar en Google Drive\")\n",
    "        print(\"ğŸ’¡ PodrÃ¡s usarlo en esta sesiÃ³n, pero se perderÃ¡ al reiniciar\")\n",
    "        \n",
    "elif SKIP_VECTORSTORE:\n",
    "    print(\"âš¡ SALTANDO: Vector store ya existÃ­a en Google Drive\")\n",
    "else:\n",
    "    print(\"âŒ No hay vector store para guardar (no se creÃ³ correctamente)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ” ConfiguraciÃ³n del Retriever para bÃºsquedas semÃ¡nticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ Configurando retriever para bÃºsquedas semÃ¡nticas...\")\n",
    "\n",
    "if vectorstore is not None:\n",
    "    # 1. Retriever bÃ¡sico por similitud\n",
    "    basic_retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5} \n",
    "    )\n",
    "    \n",
    "    # 2. Retriever con threshold de similitud (mÃ¡s estricto)\n",
    "    similarity_retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={\"k\": 10, \"score_threshold\": 0.5}\n",
    "    )\n",
    "    \n",
    "    # 3. Retriever con MMR (Maximum Marginal Relevance) para diversidad\n",
    "    mmr_retriever = vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\n",
    "            \"k\": 5,\n",
    "            \"fetch_k\": 20,              # Buscar entre los top 20\n",
    "            \"lambda_mult\": 0.7          # Balance relevancia vs diversidad\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Retrievers configurados:\")\n",
    "    print(\"   ğŸ” basic_retriever: Top 5 chunks mÃ¡s similares\")\n",
    "    print(\"   ğŸ¯ similarity_retriever: Chunks con similitud > 0.7\")\n",
    "    print(\"   ğŸŒˆ mmr_retriever: Top 5 con diversidad (MMR)\")\n",
    "    \n",
    "    def search_with_filters(query, level=None, module=None, content_type=None, k=5):\n",
    "        filter_dict = {}\n",
    "        \n",
    "        if level:\n",
    "            filter_dict['level'] = level\n",
    "        if module:\n",
    "            filter_dict['module'] = module\n",
    "        if content_type:\n",
    "            filter_dict['content_type'] = content_type\n",
    "        \n",
    "        if filter_dict:\n",
    "            results = vectorstore.similarity_search_with_score(query, k=k, filter=filter_dict)\n",
    "        else:\n",
    "            results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    print(\"âœ… FunciÃ³n de bÃºsqueda con filtros configurada\")\n",
    "    print(\"ğŸ’¡ Usa search_with_filters(query, level='intermedio', module=2, k=3)\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No se puede configurar retriever: vector store no disponible\")\n",
    "    print(\"ğŸ”„ AsegÃºrate de haber creado o cargado el vector store correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§ª Pruebas del Vector Store con consultas sobre inversiÃ³n en dividendos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§ª Probando el vector store con consultas de ejemplo...\")\n",
    "\n",
    "if vectorstore is not None:\n",
    "    test_queries = [\n",
    "        \"Â¿QuÃ© son los dividendos y por quÃ© las empresas los pagan?\",\n",
    "        \"Â¿CuÃ¡les son las ventajas de invertir en dividendos?\",\n",
    "        \"Â¿CÃ³mo seleccionar acciones que paguen dividendos?\",\n",
    "        \"Â¿QuÃ© es la rentabilidad por dividendo?\",\n",
    "        \"Â¿CuÃ¡les son los riesgos de la inversiÃ³n en dividendos?\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"ğŸ“‹ Ejecutando {len(test_queries)} consultas de prueba...\\n\")\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"ğŸ” Consulta {i}: '{query}'\")\n",
    "        \n",
    "        try:\n",
    "            results = vectorstore.similarity_search_with_score(query, k=3)\n",
    "            \n",
    "            print(f\"   ğŸ“Š Encontrados {len(results)} chunks relevantes:\")\n",
    "            \n",
    "            for j, (doc, score) in enumerate(results, 1):\n",
    "                print(f\"   {j}. ğŸ“„ {doc.metadata.get('filename', 'N/A')[:50]}...\")\n",
    "                print(f\"      ğŸ¯ Tema: {doc.metadata.get('main_topic', 'N/A')}\")\n",
    "                print(f\"      ğŸ“Š Nivel: {doc.metadata.get('level', 'N/A')}\")\n",
    "                print(f\"      ğŸ”¢ Score: {score:.3f}\")\n",
    "                print(f\"      ğŸ“ Contenido: {doc.page_content[:100]}...\")\n",
    "                print()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error en bÃºsqueda: {e}\")\n",
    "            \n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    print(\"\\nğŸ¯ Prueba con filtros por metadatos:\")\n",
    "    print(\"ğŸ” BÃºsqueda: 'dividendos' solo en contenido intermedio...\")\n",
    "    \n",
    "    try:\n",
    "        filtered_results = search_with_filters(\"dividendos\", level=\"intermedio\", k=3)\n",
    "        \n",
    "        print(f\"   ğŸ“Š Encontrados {len(filtered_results)} chunks intermedios:\")\n",
    "        for i, (doc, score) in enumerate(filtered_results, 1):\n",
    "            print(f\"   {i}. ğŸ“„ {doc.metadata.get('filename', 'N/A')}\")\n",
    "            print(f\"      ğŸ“Š Nivel: {doc.metadata.get('level', 'N/A')}\")\n",
    "            print(f\"      ğŸ“š MÃ³dulo: {doc.metadata.get('module', 'N/A')}\")\n",
    "            print(f\"      ğŸ”¢ Score: {score:.3f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error en bÃºsqueda filtrada: {e}\")\n",
    "    \n",
    "    print(\"\\nâœ… Â¡Vector store funcionando correctamente!\")\n",
    "    print(\"ğŸ’¡ Listo para implementar el pipeline RAG completo\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No se pueden ejecutar pruebas: vector store no disponible\")\n",
    "    print(\"ğŸ”„ AsegÃºrate de haber creado o cargado el vector store correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ¤– Pipeline RAG Completo\n",
    "\n",
    "Implementaremos el sistema completo de pregunta-respuesta combinando retrieval semÃ¡ntico con generaciÃ³n de respuestas usando GPT-4o-mini.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ”§ ConfiguraciÃ³n del LLM y Prompt Template especializado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ Configurando LLM y prompt template para finanzas...\")\n",
    "\n",
    "if vectorstore is not None:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "    from langchain_core.runnables import RunnablePassthrough\n",
    "    \n",
    "    base_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1, max_tokens=1000)\n",
    "    \n",
    "    financial_prompt_template = \"\"\"\n",
    "    Eres un asistente experto en inversiÃ³n en dividendos y anÃ¡lisis financiero. Tu objetivo es proporcionar respuestas precisas, educativas y prÃ¡cticas basÃ¡ndose Ãºnicamente en el contexto proporcionado.\n",
    "\n",
    "    INSTRUCCIONES:\n",
    "    1. Responde ÃšNICAMENTE basÃ¡ndote en el contexto proporcionado\n",
    "    2. Si la informaciÃ³n no estÃ¡ en el contexto, responde de forma BREVE usando una de estas variaciones:\n",
    "        - \"No tengo informaciÃ³n sobre [tema] en mi base de conocimiento financiero.\"\n",
    "        - \"Esta consulta estÃ¡ fuera de mi especializaciÃ³n en dividendos.\"\n",
    "        - \"No encuentro informaciÃ³n sobre [tema] en el contexto proporcionado.\"\n",
    "        - \"Mi conocimiento se limita a inversiÃ³n en dividendos.\"\n",
    "        - \"No puedo ayudar con [tema], solo con temas financieros.\"\n",
    "    3. Usa un lenguaje claro y profesional apropiado para inversores\n",
    "    4. Incluye ejemplos prÃ¡cticos cuando sea relevante\n",
    "    5. Estructura tu respuesta de forma clara y organizada\n",
    "    6. Si mencionas conceptos tÃ©cnicos, explÃ­calos brevemente\n",
    "    \n",
    "    CONTEXTO RELEVANTE:\n",
    "    {context}\n",
    "\n",
    "    PREGUNTA DEL USUARIO:\n",
    "    {question}\n",
    "\n",
    "    RESPUESTA EXPERTA:\n",
    "    \"\"\"\n",
    "\n",
    "    financial_prompt = PromptTemplate(template=financial_prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    \n",
    "    print(\"âœ… LLM configurado:\")\n",
    "    print(f\"   ğŸ¤– Modelo: {base_model.model_name}\")\n",
    "    print(f\"   ğŸŒ¡ï¸ Temperature: {base_model.temperature}\")\n",
    "    print(f\"   ğŸ“ Max tokens: {base_model.max_tokens}\")\n",
    "    \n",
    "    print(\"âœ… Prompt template especializado en finanzas creado\")\n",
    "    print(\"ğŸ’¡ Optimizado para respuestas educativas y precisas sobre inversiÃ³n en dividendos\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No se puede configurar el pipeline RAG: vector store no disponible\")\n",
    "    print(\"ğŸ”„ AsegÃºrate de haber creado o cargado el vector store correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ”— CreaciÃ³n de la cadena RAG (Retrieval + Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”— Creando cadena RAG completa...\")\n",
    "\n",
    "if vectorstore is not None and 'base_model' in locals():\n",
    "    rag_chain = (\n",
    "        {\"context\": basic_retriever, \"question\": RunnablePassthrough()}\n",
    "        | financial_prompt\n",
    "        | base_model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    rag_chain_similarity = (\n",
    "        {\"context\": similarity_retriever, \"question\": RunnablePassthrough()}\n",
    "        | financial_prompt\n",
    "        | base_model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    rag_chain_mmr = (\n",
    "        {\"context\": mmr_retriever, \"question\": RunnablePassthrough()}\n",
    "        | financial_prompt\n",
    "        | base_model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Cadenas RAG creadas exitosamente:\")\n",
    "    print(\"   ğŸ”— rag_chain: Cadena principal (retriever bÃ¡sico)\")\n",
    "    print(\"   ğŸ¯ rag_chain_similarity: Con threshold de similitud\")\n",
    "    print(\"   ğŸŒˆ rag_chain_mmr: Con diversidad (MMR)\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ CaracterÃ­sticas de las cadenas:\")\n",
    "    print(\"   ğŸš€ MÃ©todo: LCEL (LangChain Expression Language)\")\n",
    "    print(\"   âš¡ MÃ¡s rÃ¡pido y eficiente que RetrievalQA\")\n",
    "    print(\"   ğŸ¯ Prompt: Especializado en finanzas\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No se pueden crear las cadenas RAG\")\n",
    "    if vectorstore is None:\n",
    "        print(\"   - Vector store no disponible\")\n",
    "    if 'base_model' not in locals():\n",
    "        print(\"   - LLM no configurado\")\n",
    "    print(\"ğŸ”„ AsegÃºrate de haber ejecutado las celdas anteriores correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¯ Funciones helper para consultas especializadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ Creando funciones helper para consultas...\")\n",
    "\n",
    "if 'rag_chain' in locals():\n",
    "    \n",
    "    def ask_dividends_expert(question, retriever_type=\"basic\", show_sources=True):\n",
    "        if retriever_type == \"similarity\":\n",
    "            chain = rag_chain_similarity\n",
    "            retriever = similarity_retriever\n",
    "        elif retriever_type == \"mmr\":\n",
    "            chain = rag_chain_mmr\n",
    "            retriever = mmr_retriever\n",
    "        else:\n",
    "            chain = rag_chain\n",
    "            retriever = basic_retriever\n",
    "        \n",
    "        try:\n",
    "            answer = chain.invoke(question)\n",
    "\n",
    "            sources = []\n",
    "            if show_sources:\n",
    "                sources = retriever.invoke(question)\n",
    "            \n",
    "            response = {\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"sources\": sources,\n",
    "                \"retriever_type\": retriever_type\n",
    "            }\n",
    "            \n",
    "            if show_sources:\n",
    "                print(f\"ğŸ¤– **RESPUESTA DEL EXPERTO:**\")\n",
    "                print(f\"{answer}\")\n",
    "                print(f\"\\nğŸ“š **FUENTES CONSULTADAS ({len(response['sources'])} documentos):**\")\n",
    "                \n",
    "                for i, doc in enumerate(response['sources'], 1):\n",
    "                    print(f\"\\n{i}. ğŸ“„ **{doc.metadata.get('filename', 'N/A')}**\")\n",
    "                    print(f\"   ğŸ¯ Tema: {doc.metadata.get('main_topic', 'N/A')}\")\n",
    "                    print(f\"   ğŸ“Š Nivel: {doc.metadata.get('level', 'N/A')}\")\n",
    "                    print(f\"   ğŸ“š MÃ³dulo: {doc.metadata.get('module', 'N/A')}\")\n",
    "                    print(f\"   ğŸ“ Extracto: {doc.page_content[:150]}...\")\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error ejecutando consulta: {e}\")\n",
    "            return None\n",
    "    \n",
    "    print(\"âœ… FunciÃ³n helper creada:\")\n",
    "    print(\"   ğŸ¤– ask_dividends_expert(): Consulta principal\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ Ejemplo de uso:\")\n",
    "    print('   ask_dividends_expert(\"Â¿QuÃ© son los dividendos?\")')\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No se puede crear la funciÃ³n helper: cadena RAG no disponible\")\n",
    "    print(\"ğŸ”„ AsegÃºrate de haber ejecutado las celdas anteriores correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§ª Pruebas completas del Sistema RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§ª Probando el sistema RAG completo...\")\n",
    "\n",
    "if 'ask_dividends_expert' in locals():\n",
    "    test_questions = [\n",
    "        {\n",
    "            \"question\": \"Â¿QuÃ© son los dividendos?\",\n",
    "            \"description\": \"Pregunta bÃ¡sica sobre conceptos fundamentales\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Â¿CuÃ¡les son las principales ventajas de invertir en dividendos?\",\n",
    "            \"description\": \"Pregunta sobre beneficios de la estrategia\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Â¿CÃ³mo puedo evaluar si una empresa paga dividendos sostenibles?\",\n",
    "            \"description\": \"Pregunta prÃ¡ctica sobre anÃ¡lisis\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Â¿CuÃ¡les son los principales ratios financieros para evaluar empresas que reparten dividendos?\",\n",
    "            \"description\": \"Pregunta sobre ratios financieros\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Â¿QuÃ© riesgos debo considerar al invertir en dividendos?\",\n",
    "            \"description\": \"Pregunta sobre gestiÃ³n de riesgo\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"ğŸ¯ Ejecutando {len(test_questions)} pruebas del sistema RAG...\\n\")\n",
    "    \n",
    "    for i, test in enumerate(test_questions, 1):\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"ğŸ” PRUEBA {i}: {test['description']}\")\n",
    "        print(f\"â“ PREGUNTA: {test['question']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            result = ask_dividends_expert(\n",
    "                test['question'], \n",
    "                retriever_type=\"similarity\",\n",
    "                show_sources=True\n",
    "            )\n",
    "            \n",
    "            if result:\n",
    "                print(f\"\\nâœ… Consulta ejecutada exitosamente\")\n",
    "                print(f\"ğŸ“Š Fuentes utilizadas: {len(result['sources'])}\")\n",
    "            else:\n",
    "                print(f\"\\nâŒ Error en la consulta\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Error ejecutando prueba {i}: {e}\")\n",
    "        \n",
    "        print(f\"\\n{'-'*80}\\n\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ğŸ‰ Â¡SISTEMA RAG COMPLETAMENTE FUNCIONAL!\")\n",
    "    print(\"ğŸ’¡ El asistente de inversiÃ³n en dividendos estÃ¡ listo para usar\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No se pueden ejecutar pruebas: funciones RAG no disponibles\")\n",
    "    print(\"ğŸ”„ AsegÃºrate de haber ejecutado todas las celdas anteriores correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§ª VerificaciÃ³n de que el sistema usa SOLO el RAG (no conocimiento externo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§ª Verificando que el sistema usa ÃšNICAMENTE el RAG...\")\n",
    "\n",
    "if 'ask_dividends_expert' in locals():\n",
    "    non_rag_questions = [\n",
    "        {\n",
    "            \"question\": \"Â¿QuiÃ©n es Lionel Messi?\",\n",
    "            \"expected\": \"No deberÃ­a saber nada sobre fÃºtbol\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Â¿CÃ³mo funciona la inteligencia artificial?\",\n",
    "            \"expected\": \"No deberÃ­a tener info sobre IA general\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Â¿CuÃ¡l es la capital de Francia?\",\n",
    "            \"expected\": \"No deberÃ­a saber geografÃ­a\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Â¿QuÃ© es Python en programaciÃ³n?\",\n",
    "            \"expected\": \"No deberÃ­a saber sobre programaciÃ³n\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Â¿CÃ³mo se hace una paella?\",\n",
    "            \"expected\": \"No deberÃ­a saber cocina\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"ğŸ¯ Ejecutando {len(non_rag_questions)} preguntas FUERA del dominio financiero...\\n\")\n",
    "    \n",
    "    for i, test in enumerate(non_rag_questions, 1):\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"ğŸ” PRUEBA ANTI-RAG {i}: {test['expected']}\")\n",
    "        print(f\"â“ PREGUNTA: {test['question']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            result = ask_dividends_expert(\n",
    "                test['question'], \n",
    "                retriever_type=\"similarity\",\n",
    "                show_sources=True\n",
    "            )\n",
    "            \n",
    "            if result:\n",
    "                print(f\"\\nâœ… Consulta ejecutada exitosamente\")\n",
    "                print(f\"ğŸ“Š Fuentes utilizadas: {len(result['sources'])}\")\n",
    "            else:\n",
    "                print(f\"\\nâŒ Error en la consulta\")\n",
    "            \n",
    "            no_info_indicators = [\n",
    "                \"no tengo informaciÃ³n\",\n",
    "                \"no estÃ¡ en el contexto\",\n",
    "                \"no puedo responder\",\n",
    "                \"informaciÃ³n no disponible\",\n",
    "                \"no se encuentra\",\n",
    "                \"no hay informaciÃ³n\"\n",
    "            ]\n",
    "            \n",
    "            has_no_info = any(indicator in result['answer'].lower() for indicator in no_info_indicators)\n",
    "            \n",
    "            if has_no_info:\n",
    "                print(f\"âœ… **CORRECTO**: El sistema indica que no tiene la informaciÃ³n\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ **ATENCIÃ“N**: El sistema podrÃ­a estar usando conocimiento externo\")\n",
    "                print(f\"ğŸ’¡ Revisar si el prompt estÃ¡ funcionando correctamente\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error ejecutando prueba anti-RAG {i}: {e}\")\n",
    "        \n",
    "        print(f\"\\n{'-'*80}\\n\")\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"ğŸ¯ **RESUMEN DE VERIFICACIÃ“N:**\")\n",
    "    print(\"âœ… Si todas las respuestas indican 'no tengo informaciÃ³n' â†’ RAG funciona correctamente\")\n",
    "    print(\"âš ï¸ Si alguna respuesta da informaciÃ³n externa â†’ Revisar configuraciÃ³n del prompt\")\n",
    "    print(\"ğŸ’¡ Un buen sistema RAG debe decir 'no sÃ©' cuando no tiene la informaciÃ³n en su base de conocimiento\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No se puede ejecutar la verificaciÃ³n: funciÃ³n RAG no disponible\")\n",
    "    print(\"ğŸ”„ AsegÃºrate de haber ejecutado todas las celdas anteriores correctamente\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
